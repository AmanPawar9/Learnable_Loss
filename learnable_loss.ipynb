{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958090c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be37b969",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8899f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Plots will be saved in 'plots_pytorch/' directory.\n",
      "\n",
      "--- Processing Dataset: MNIST (PyTorch) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:18<00:00, 545kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 66.1kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:06<00:00, 245kB/s] \n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.88MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: mnist with 60000 training samples and 10000 test samples.\n",
      "\n",
      "--- Run 1/3 ---\n",
      "Epoch [1/20] Train Loss: 0.4648 Acc: 0.8529 | Val Loss: 0.0842 Acc: 0.9744\n",
      "Epoch [2/20] Train Loss: 0.1771 Acc: 0.9463 | Val Loss: 0.0534 Acc: 0.9823\n",
      "Epoch [3/20] Train Loss: 0.1306 Acc: 0.9604 | Val Loss: 0.0394 Acc: 0.9869\n",
      "Epoch [4/20] Train Loss: 0.1081 Acc: 0.9667 | Val Loss: 0.0380 Acc: 0.9872\n",
      "Epoch [5/20] Train Loss: 0.0975 Acc: 0.9698 | Val Loss: 0.0359 Acc: 0.9894\n",
      "Epoch [6/20] Train Loss: 0.0862 Acc: 0.9736 | Val Loss: 0.0315 Acc: 0.9903\n",
      "Epoch [7/20] Train Loss: 0.0789 Acc: 0.9754 | Val Loss: 0.0324 Acc: 0.9902\n",
      "Epoch [8/20] Train Loss: 0.0738 Acc: 0.9769 | Val Loss: 0.0334 Acc: 0.9893\n",
      "Epoch [9/20] Train Loss: 0.0638 Acc: 0.9791 | Val Loss: 0.0314 Acc: 0.9913\n",
      "Epoch [10/20] Train Loss: 0.0578 Acc: 0.9814 | Val Loss: 0.0279 Acc: 0.9913\n",
      "Epoch [11/20] Train Loss: 0.0517 Acc: 0.9823 | Val Loss: 0.0266 Acc: 0.9909\n",
      "Epoch [12/20] Train Loss: 0.0478 Acc: 0.9837 | Val Loss: 0.0289 Acc: 0.9912\n",
      "Epoch [13/20] Train Loss: 0.0455 Acc: 0.9842 | Val Loss: 0.0299 Acc: 0.9911\n",
      "Epoch [14/20] Train Loss: 0.0435 Acc: 0.9844 | Val Loss: 0.0324 Acc: 0.9910\n",
      "Epoch [15/20] Train Loss: 0.0410 Acc: 0.9860 | Val Loss: 0.0278 Acc: 0.9915\n",
      "Epoch [16/20] Train Loss: 0.0398 Acc: 0.9860 | Val Loss: 0.0273 Acc: 0.9926\n",
      "Epoch [17/20] Train Loss: 0.0369 Acc: 0.9870 | Val Loss: 0.0295 Acc: 0.9925\n",
      "Epoch [18/20] Train Loss: 0.0363 Acc: 0.9874 | Val Loss: 0.0253 Acc: 0.9929\n",
      "Epoch [19/20] Train Loss: 0.0326 Acc: 0.9886 | Val Loss: 0.0303 Acc: 0.9928\n",
      "Epoch [20/20] Train Loss: 0.0327 Acc: 0.9886 | Val Loss: 0.0282 Acc: 0.9925\n",
      "Run 1 completed in 95.43 seconds. Final Test Accuracy: 0.9925\n",
      "\n",
      "--- Run 2/3 ---\n",
      "Epoch [1/20] Train Loss: 0.5018 Acc: 0.8395 | Val Loss: 0.0893 Acc: 0.9711\n",
      "Epoch [2/20] Train Loss: 0.1924 Acc: 0.9414 | Val Loss: 0.0553 Acc: 0.9814\n",
      "Epoch [3/20] Train Loss: 0.1404 Acc: 0.9570 | Val Loss: 0.0503 Acc: 0.9840\n",
      "Epoch [4/20] Train Loss: 0.1158 Acc: 0.9654 | Val Loss: 0.0399 Acc: 0.9857\n",
      "Epoch [5/20] Train Loss: 0.1009 Acc: 0.9692 | Val Loss: 0.0385 Acc: 0.9874\n",
      "Epoch [6/20] Train Loss: 0.0913 Acc: 0.9716 | Val Loss: 0.0325 Acc: 0.9897\n",
      "Epoch [7/20] Train Loss: 0.0820 Acc: 0.9739 | Val Loss: 0.0327 Acc: 0.9891\n",
      "Epoch [8/20] Train Loss: 0.0758 Acc: 0.9757 | Val Loss: 0.0289 Acc: 0.9903\n",
      "Epoch [9/20] Train Loss: 0.0682 Acc: 0.9786 | Val Loss: 0.0247 Acc: 0.9916\n",
      "Epoch [10/20] Train Loss: 0.0656 Acc: 0.9791 | Val Loss: 0.0265 Acc: 0.9912\n",
      "Epoch [11/20] Train Loss: 0.0601 Acc: 0.9816 | Val Loss: 0.0254 Acc: 0.9919\n",
      "Epoch [12/20] Train Loss: 0.0546 Acc: 0.9826 | Val Loss: 0.0276 Acc: 0.9915\n",
      "Epoch [13/20] Train Loss: 0.0544 Acc: 0.9824 | Val Loss: 0.0264 Acc: 0.9915\n",
      "Epoch [14/20] Train Loss: 0.0496 Acc: 0.9842 | Val Loss: 0.0251 Acc: 0.9930\n",
      "Epoch [15/20] Train Loss: 0.0480 Acc: 0.9844 | Val Loss: 0.0260 Acc: 0.9929\n",
      "Epoch [16/20] Train Loss: 0.0442 Acc: 0.9857 | Val Loss: 0.0229 Acc: 0.9928\n",
      "Epoch [17/20] Train Loss: 0.0433 Acc: 0.9857 | Val Loss: 0.0244 Acc: 0.9929\n",
      "Epoch [18/20] Train Loss: 0.0408 Acc: 0.9866 | Val Loss: 0.0276 Acc: 0.9920\n",
      "Epoch [19/20] Train Loss: 0.0372 Acc: 0.9873 | Val Loss: 0.0228 Acc: 0.9930\n",
      "Epoch [20/20] Train Loss: 0.0361 Acc: 0.9875 | Val Loss: 0.0233 Acc: 0.9939\n",
      "Run 2 completed in 93.67 seconds. Final Test Accuracy: 0.9939\n",
      "\n",
      "--- Run 3/3 ---\n",
      "Epoch [1/20] Train Loss: 0.4873 Acc: 0.8452 | Val Loss: 0.0862 Acc: 0.9732\n",
      "Epoch [2/20] Train Loss: 0.1923 Acc: 0.9422 | Val Loss: 0.0531 Acc: 0.9827\n",
      "Epoch [3/20] Train Loss: 0.1459 Acc: 0.9559 | Val Loss: 0.0410 Acc: 0.9858\n",
      "Epoch [4/20] Train Loss: 0.1166 Acc: 0.9641 | Val Loss: 0.0361 Acc: 0.9879\n",
      "Epoch [5/20] Train Loss: 0.0998 Acc: 0.9694 | Val Loss: 0.0335 Acc: 0.9885\n",
      "Epoch [6/20] Train Loss: 0.0912 Acc: 0.9725 | Val Loss: 0.0321 Acc: 0.9898\n",
      "Epoch [7/20] Train Loss: 0.0779 Acc: 0.9756 | Val Loss: 0.0284 Acc: 0.9906\n",
      "Epoch [8/20] Train Loss: 0.0731 Acc: 0.9775 | Val Loss: 0.0292 Acc: 0.9905\n",
      "Epoch [9/20] Train Loss: 0.0656 Acc: 0.9792 | Val Loss: 0.0279 Acc: 0.9912\n",
      "Epoch [10/20] Train Loss: 0.0606 Acc: 0.9801 | Val Loss: 0.0281 Acc: 0.9918\n",
      "Epoch [11/20] Train Loss: 0.0574 Acc: 0.9816 | Val Loss: 0.0264 Acc: 0.9924\n",
      "Epoch [12/20] Train Loss: 0.0531 Acc: 0.9830 | Val Loss: 0.0265 Acc: 0.9925\n",
      "Epoch [13/20] Train Loss: 0.0518 Acc: 0.9839 | Val Loss: 0.0255 Acc: 0.9928\n",
      "Epoch [14/20] Train Loss: 0.0460 Acc: 0.9847 | Val Loss: 0.0257 Acc: 0.9927\n",
      "Epoch [15/20] Train Loss: 0.0470 Acc: 0.9843 | Val Loss: 0.0264 Acc: 0.9919\n",
      "Epoch [16/20] Train Loss: 0.0423 Acc: 0.9856 | Val Loss: 0.0293 Acc: 0.9912\n",
      "Epoch [17/20] Train Loss: 0.0391 Acc: 0.9869 | Val Loss: 0.0280 Acc: 0.9926\n",
      "Epoch [18/20] Train Loss: 0.0384 Acc: 0.9871 | Val Loss: 0.0284 Acc: 0.9928\n",
      "Epoch [19/20] Train Loss: 0.0365 Acc: 0.9874 | Val Loss: 0.0278 Acc: 0.9922\n",
      "Epoch [20/20] Train Loss: 0.0374 Acc: 0.9878 | Val Loss: 0.0276 Acc: 0.9923\n",
      "Run 3 completed in 95.91 seconds. Final Test Accuracy: 0.9923\n",
      "\n",
      "--- MNIST Final Results (3 Runs) (PyTorch) ---\n",
      "Individual Test Accuracies: ['0.9925', '0.9939', '0.9923']\n",
      "Average Test Accuracy: 0.9929\n",
      "Standard Deviation of Test Accuracy: 0.0007\n",
      "Average Run Time: 95.00 seconds\n",
      "Total Time for MNIST: 319.50 seconds\n",
      "Accuracy plot saved to: plots_pytorch/results_mnist_accuracy_plot_pytorch.png\n",
      "Loss plot saved to: plots_pytorch/results_mnist_loss_log_plot_pytorch.png\n",
      "\n",
      "=== Additional Classification Metrics (sklearn) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9949    0.9939    0.9944       980\n",
      "           1     0.9956    0.9965    0.9960      1135\n",
      "           2     0.9903    0.9903    0.9903      1032\n",
      "           3     0.9901    0.9941    0.9921      1010\n",
      "           4     0.9929    0.9949    0.9939       982\n",
      "           5     0.9910    0.9922    0.9916       892\n",
      "           6     0.9886    0.9927    0.9906       958\n",
      "           7     0.9932    0.9893    0.9912      1028\n",
      "           8     0.9898    0.9918    0.9908       974\n",
      "           9     0.9960    0.9871    0.9915      1009\n",
      "\n",
      "    accuracy                         0.9923     10000\n",
      "   macro avg     0.9922    0.9923    0.9922     10000\n",
      "weighted avg     0.9923    0.9923    0.9923     10000\n",
      "\n",
      "\n",
      "--- Processing Dataset: FASHION_MNIST (PyTorch) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.3MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 213kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.40MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 11.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: fashion_mnist with 60000 training samples and 10000 test samples.\n",
      "\n",
      "--- Run 1/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 0.7566 Acc: 0.7235 | Val Loss: 0.4328 Acc: 0.8417\n",
      "Epoch [2/20] Train Loss: 0.5082 Acc: 0.8138 | Val Loss: 0.3752 Acc: 0.8642\n",
      "Epoch [3/20] Train Loss: 0.4482 Acc: 0.8357 | Val Loss: 0.3309 Acc: 0.8792\n",
      "Epoch [4/20] Train Loss: 0.4087 Acc: 0.8510 | Val Loss: 0.3118 Acc: 0.8864\n",
      "Epoch [5/20] Train Loss: 0.3836 Acc: 0.8590 | Val Loss: 0.2947 Acc: 0.8942\n",
      "Epoch [6/20] Train Loss: 0.3608 Acc: 0.8659 | Val Loss: 0.2827 Acc: 0.8974\n",
      "Epoch [7/20] Train Loss: 0.3439 Acc: 0.8721 | Val Loss: 0.2755 Acc: 0.8982\n",
      "Epoch [8/20] Train Loss: 0.3276 Acc: 0.8786 | Val Loss: 0.2649 Acc: 0.9034\n",
      "Epoch [9/20] Train Loss: 0.3124 Acc: 0.8840 | Val Loss: 0.2823 Acc: 0.8976\n",
      "Epoch [10/20] Train Loss: 0.3020 Acc: 0.8871 | Val Loss: 0.2566 Acc: 0.9082\n",
      "Epoch [11/20] Train Loss: 0.2894 Acc: 0.8931 | Val Loss: 0.2446 Acc: 0.9113\n",
      "Epoch [12/20] Train Loss: 0.2783 Acc: 0.8948 | Val Loss: 0.2458 Acc: 0.9111\n",
      "Epoch [13/20] Train Loss: 0.2679 Acc: 0.8999 | Val Loss: 0.2463 Acc: 0.9097\n",
      "Epoch [14/20] Train Loss: 0.2597 Acc: 0.9036 | Val Loss: 0.2491 Acc: 0.9095\n",
      "Epoch [15/20] Train Loss: 0.2533 Acc: 0.9049 | Val Loss: 0.2428 Acc: 0.9127\n",
      "Epoch [16/20] Train Loss: 0.2434 Acc: 0.9095 | Val Loss: 0.2520 Acc: 0.9135\n",
      "Epoch [17/20] Train Loss: 0.2397 Acc: 0.9099 | Val Loss: 0.2461 Acc: 0.9174\n",
      "Epoch [18/20] Train Loss: 0.2318 Acc: 0.9135 | Val Loss: 0.2405 Acc: 0.9172\n",
      "Epoch [19/20] Train Loss: 0.2247 Acc: 0.9148 | Val Loss: 0.2372 Acc: 0.9155\n",
      "Epoch [20/20] Train Loss: 0.2233 Acc: 0.9144 | Val Loss: 0.2405 Acc: 0.9170\n",
      "Run 1 completed in 99.01 seconds. Final Test Accuracy: 0.9170\n",
      "\n",
      "--- Run 2/3 ---\n",
      "Epoch [1/20] Train Loss: 0.7347 Acc: 0.7329 | Val Loss: 0.4205 Acc: 0.8496\n",
      "Epoch [2/20] Train Loss: 0.4789 Acc: 0.8270 | Val Loss: 0.3584 Acc: 0.8674\n",
      "Epoch [3/20] Train Loss: 0.4200 Acc: 0.8495 | Val Loss: 0.3348 Acc: 0.8782\n",
      "Epoch [4/20] Train Loss: 0.3837 Acc: 0.8627 | Val Loss: 0.3006 Acc: 0.8901\n",
      "Epoch [5/20] Train Loss: 0.3532 Acc: 0.8721 | Val Loss: 0.2929 Acc: 0.8897\n",
      "Epoch [6/20] Train Loss: 0.3356 Acc: 0.8788 | Val Loss: 0.2754 Acc: 0.8984\n",
      "Epoch [7/20] Train Loss: 0.3176 Acc: 0.8850 | Val Loss: 0.2759 Acc: 0.8990\n",
      "Epoch [8/20] Train Loss: 0.3010 Acc: 0.8914 | Val Loss: 0.2684 Acc: 0.9026\n",
      "Epoch [9/20] Train Loss: 0.2873 Acc: 0.8955 | Val Loss: 0.2597 Acc: 0.9089\n",
      "Epoch [10/20] Train Loss: 0.2768 Acc: 0.8995 | Val Loss: 0.2487 Acc: 0.9087\n",
      "Epoch [11/20] Train Loss: 0.2675 Acc: 0.9012 | Val Loss: 0.2477 Acc: 0.9117\n",
      "Epoch [12/20] Train Loss: 0.2552 Acc: 0.9057 | Val Loss: 0.2538 Acc: 0.9117\n",
      "Epoch [13/20] Train Loss: 0.2474 Acc: 0.9094 | Val Loss: 0.2435 Acc: 0.9152\n",
      "Epoch [14/20] Train Loss: 0.2361 Acc: 0.9130 | Val Loss: 0.2425 Acc: 0.9156\n",
      "Epoch [15/20] Train Loss: 0.2294 Acc: 0.9147 | Val Loss: 0.2346 Acc: 0.9170\n",
      "Epoch [16/20] Train Loss: 0.2244 Acc: 0.9178 | Val Loss: 0.2382 Acc: 0.9155\n",
      "Epoch [17/20] Train Loss: 0.2167 Acc: 0.9189 | Val Loss: 0.2380 Acc: 0.9197\n",
      "Epoch [18/20] Train Loss: 0.2103 Acc: 0.9210 | Val Loss: 0.2383 Acc: 0.9173\n",
      "Epoch [19/20] Train Loss: 0.2010 Acc: 0.9243 | Val Loss: 0.2367 Acc: 0.9231\n",
      "Epoch [20/20] Train Loss: 0.1950 Acc: 0.9260 | Val Loss: 0.2462 Acc: 0.9219\n",
      "Run 2 completed in 98.33 seconds. Final Test Accuracy: 0.9219\n",
      "\n",
      "--- Run 3/3 ---\n",
      "Epoch [1/20] Train Loss: 0.7584 Acc: 0.7240 | Val Loss: 0.4447 Acc: 0.8298\n",
      "Epoch [2/20] Train Loss: 0.5009 Acc: 0.8176 | Val Loss: 0.3591 Acc: 0.8675\n",
      "Epoch [3/20] Train Loss: 0.4423 Acc: 0.8409 | Val Loss: 0.3266 Acc: 0.8806\n",
      "Epoch [4/20] Train Loss: 0.4078 Acc: 0.8526 | Val Loss: 0.3151 Acc: 0.8840\n",
      "Epoch [5/20] Train Loss: 0.3814 Acc: 0.8609 | Val Loss: 0.3096 Acc: 0.8868\n",
      "Epoch [6/20] Train Loss: 0.3601 Acc: 0.8698 | Val Loss: 0.2820 Acc: 0.8980\n",
      "Epoch [7/20] Train Loss: 0.3434 Acc: 0.8762 | Val Loss: 0.2906 Acc: 0.8961\n",
      "Epoch [8/20] Train Loss: 0.3287 Acc: 0.8800 | Val Loss: 0.2631 Acc: 0.9043\n",
      "Epoch [9/20] Train Loss: 0.3170 Acc: 0.8844 | Val Loss: 0.2645 Acc: 0.9091\n",
      "Epoch [10/20] Train Loss: 0.3014 Acc: 0.8893 | Val Loss: 0.2684 Acc: 0.9014\n",
      "Epoch [11/20] Train Loss: 0.2954 Acc: 0.8920 | Val Loss: 0.2497 Acc: 0.9119\n",
      "Epoch [12/20] Train Loss: 0.2832 Acc: 0.8960 | Val Loss: 0.2664 Acc: 0.9023\n",
      "Epoch [13/20] Train Loss: 0.2760 Acc: 0.8978 | Val Loss: 0.2463 Acc: 0.9110\n",
      "Epoch [14/20] Train Loss: 0.2670 Acc: 0.9021 | Val Loss: 0.2477 Acc: 0.9159\n",
      "Epoch [15/20] Train Loss: 0.2584 Acc: 0.9039 | Val Loss: 0.2498 Acc: 0.9115\n",
      "Epoch [16/20] Train Loss: 0.2538 Acc: 0.9059 | Val Loss: 0.2419 Acc: 0.9167\n",
      "Epoch [17/20] Train Loss: 0.2472 Acc: 0.9079 | Val Loss: 0.2389 Acc: 0.9179\n",
      "Epoch [18/20] Train Loss: 0.2407 Acc: 0.9099 | Val Loss: 0.2495 Acc: 0.9158\n",
      "Epoch [19/20] Train Loss: 0.2333 Acc: 0.9132 | Val Loss: 0.2426 Acc: 0.9167\n",
      "Epoch [20/20] Train Loss: 0.2251 Acc: 0.9147 | Val Loss: 0.2440 Acc: 0.9197\n",
      "Run 3 completed in 98.03 seconds. Final Test Accuracy: 0.9197\n",
      "\n",
      "--- FASHION_MNIST Final Results (3 Runs) (PyTorch) ---\n",
      "Individual Test Accuracies: ['0.9170', '0.9219', '0.9197']\n",
      "Average Test Accuracy: 0.9195\n",
      "Standard Deviation of Test Accuracy: 0.0020\n",
      "Average Run Time: 98.46 seconds\n",
      "Total Time for FASHION_MNIST: 302.42 seconds\n",
      "Accuracy plot saved to: plots_pytorch/results_fashion_mnist_accuracy_plot_pytorch.png\n",
      "Loss plot saved to: plots_pytorch/results_fashion_mnist_loss_log_plot_pytorch.png\n",
      "\n",
      "=== Additional Classification Metrics (sklearn) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8753    0.8560    0.8655      1000\n",
      "           1     0.9959    0.9810    0.9884      1000\n",
      "           2     0.8712    0.8860    0.8785      1000\n",
      "           3     0.8822    0.9510    0.9153      1000\n",
      "           4     0.8635    0.8980    0.8804      1000\n",
      "           5     0.9831    0.9880    0.9855      1000\n",
      "           6     0.7984    0.7170    0.7555      1000\n",
      "           7     0.9479    0.9820    0.9646      1000\n",
      "           8     0.9860    0.9860    0.9860      1000\n",
      "           9     0.9886    0.9520    0.9699      1000\n",
      "\n",
      "    accuracy                         0.9197     10000\n",
      "   macro avg     0.9192    0.9197    0.9190     10000\n",
      "weighted avg     0.9192    0.9197    0.9190     10000\n",
      "\n",
      "\n",
      "--- Processing Dataset: CIFAR10 (PyTorch) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:57<00:00, 2.96MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: cifar10 with 50000 training samples and 10000 test samples.\n",
      "\n",
      "--- Run 1/3 ---\n",
      "Epoch [1/20] Train Loss: 1.8068 Acc: 0.3275 | Val Loss: 1.4730 Acc: 0.4741\n",
      "Epoch [2/20] Train Loss: 1.5411 Acc: 0.4333 | Val Loss: 1.3301 Acc: 0.5346\n",
      "Epoch [3/20] Train Loss: 1.4168 Acc: 0.4836 | Val Loss: 1.2090 Acc: 0.5748\n",
      "Epoch [4/20] Train Loss: 1.3287 Acc: 0.5153 | Val Loss: 1.1473 Acc: 0.5996\n",
      "Epoch [5/20] Train Loss: 1.2598 Acc: 0.5431 | Val Loss: 1.0937 Acc: 0.6207\n",
      "Epoch [6/20] Train Loss: 1.2123 Acc: 0.5609 | Val Loss: 1.0357 Acc: 0.6384\n",
      "Epoch [7/20] Train Loss: 1.1752 Acc: 0.5774 | Val Loss: 1.0309 Acc: 0.6338\n",
      "Epoch [8/20] Train Loss: 1.1442 Acc: 0.5874 | Val Loss: 0.9866 Acc: 0.6553\n",
      "Epoch [9/20] Train Loss: 1.1107 Acc: 0.6008 | Val Loss: 0.9609 Acc: 0.6729\n",
      "Epoch [10/20] Train Loss: 1.0872 Acc: 0.6102 | Val Loss: 0.9636 Acc: 0.6729\n",
      "Epoch [11/20] Train Loss: 1.0660 Acc: 0.6158 | Val Loss: 0.9239 Acc: 0.6787\n",
      "Epoch [12/20] Train Loss: 1.0445 Acc: 0.6223 | Val Loss: 0.9194 Acc: 0.6785\n",
      "Epoch [13/20] Train Loss: 1.0217 Acc: 0.6300 | Val Loss: 0.8989 Acc: 0.6898\n",
      "Epoch [14/20] Train Loss: 1.0009 Acc: 0.6348 | Val Loss: 0.8987 Acc: 0.6883\n",
      "Epoch [15/20] Train Loss: 0.9877 Acc: 0.6405 | Val Loss: 0.8868 Acc: 0.6928\n",
      "Epoch [16/20] Train Loss: 0.9684 Acc: 0.6476 | Val Loss: 0.8752 Acc: 0.6966\n",
      "Epoch [17/20] Train Loss: 0.9487 Acc: 0.6554 | Val Loss: 0.8704 Acc: 0.6999\n",
      "Epoch [18/20] Train Loss: 0.9305 Acc: 0.6594 | Val Loss: 0.8921 Acc: 0.6887\n",
      "Epoch [19/20] Train Loss: 0.9154 Acc: 0.6629 | Val Loss: 0.8807 Acc: 0.6972\n",
      "Epoch [20/20] Train Loss: 0.9028 Acc: 0.6660 | Val Loss: 0.8579 Acc: 0.7068\n",
      "Run 1 completed in 106.91 seconds. Final Test Accuracy: 0.7068\n",
      "\n",
      "--- Run 2/3 ---\n",
      "Epoch [1/20] Train Loss: 1.8610 Acc: 0.3068 | Val Loss: 1.5034 Acc: 0.4703\n",
      "Epoch [2/20] Train Loss: 1.5970 Acc: 0.4082 | Val Loss: 1.3462 Acc: 0.5078\n",
      "Epoch [3/20] Train Loss: 1.4905 Acc: 0.4537 | Val Loss: 1.2504 Acc: 0.5546\n",
      "Epoch [4/20] Train Loss: 1.4092 Acc: 0.4803 | Val Loss: 1.1937 Acc: 0.5707\n",
      "Epoch [5/20] Train Loss: 1.3534 Acc: 0.5027 | Val Loss: 1.1268 Acc: 0.5997\n",
      "Epoch [6/20] Train Loss: 1.3004 Acc: 0.5237 | Val Loss: 1.1007 Acc: 0.6156\n",
      "Epoch [7/20] Train Loss: 1.2681 Acc: 0.5359 | Val Loss: 1.0871 Acc: 0.6224\n",
      "Epoch [8/20] Train Loss: 1.2323 Acc: 0.5486 | Val Loss: 1.0359 Acc: 0.6396\n",
      "Epoch [9/20] Train Loss: 1.2021 Acc: 0.5612 | Val Loss: 0.9949 Acc: 0.6471\n",
      "Epoch [10/20] Train Loss: 1.1828 Acc: 0.5665 | Val Loss: 0.9898 Acc: 0.6559\n",
      "Epoch [11/20] Train Loss: 1.1617 Acc: 0.5734 | Val Loss: 0.9830 Acc: 0.6552\n",
      "Epoch [12/20] Train Loss: 1.1399 Acc: 0.5832 | Val Loss: 0.9636 Acc: 0.6631\n",
      "Epoch [13/20] Train Loss: 1.1182 Acc: 0.5919 | Val Loss: 0.9520 Acc: 0.6687\n",
      "Epoch [14/20] Train Loss: 1.1032 Acc: 0.5957 | Val Loss: 0.9319 Acc: 0.6804\n",
      "Epoch [15/20] Train Loss: 1.0854 Acc: 0.6029 | Val Loss: 0.9478 Acc: 0.6718\n",
      "Epoch [16/20] Train Loss: 1.0782 Acc: 0.6060 | Val Loss: 0.9552 Acc: 0.6736\n",
      "Epoch [17/20] Train Loss: 1.0557 Acc: 0.6134 | Val Loss: 0.9166 Acc: 0.6795\n",
      "Epoch [18/20] Train Loss: 1.0418 Acc: 0.6168 | Val Loss: 0.9187 Acc: 0.6817\n",
      "Epoch [19/20] Train Loss: 1.0219 Acc: 0.6218 | Val Loss: 0.9029 Acc: 0.6863\n",
      "Epoch [20/20] Train Loss: 1.0194 Acc: 0.6230 | Val Loss: 0.9109 Acc: 0.6849\n",
      "Run 2 completed in 105.18 seconds. Final Test Accuracy: 0.6849\n",
      "\n",
      "--- Run 3/3 ---\n",
      "Epoch [1/20] Train Loss: 1.7758 Acc: 0.3483 | Val Loss: 1.3907 Acc: 0.5065\n",
      "Epoch [2/20] Train Loss: 1.4964 Acc: 0.4543 | Val Loss: 1.2707 Acc: 0.5529\n",
      "Epoch [3/20] Train Loss: 1.3916 Acc: 0.4975 | Val Loss: 1.1940 Acc: 0.5719\n",
      "Epoch [4/20] Train Loss: 1.3049 Acc: 0.5290 | Val Loss: 1.1055 Acc: 0.6045\n",
      "Epoch [5/20] Train Loss: 1.2381 Acc: 0.5533 | Val Loss: 1.0559 Acc: 0.6359\n",
      "Epoch [6/20] Train Loss: 1.1894 Acc: 0.5718 | Val Loss: 1.0207 Acc: 0.6427\n",
      "Epoch [7/20] Train Loss: 1.1537 Acc: 0.5862 | Val Loss: 0.9889 Acc: 0.6530\n",
      "Epoch [8/20] Train Loss: 1.1114 Acc: 0.6021 | Val Loss: 0.9939 Acc: 0.6579\n",
      "Epoch [9/20] Train Loss: 1.0766 Acc: 0.6113 | Val Loss: 0.9489 Acc: 0.6685\n",
      "Epoch [10/20] Train Loss: 1.0503 Acc: 0.6202 | Val Loss: 0.9184 Acc: 0.6812\n",
      "Epoch [11/20] Train Loss: 1.0240 Acc: 0.6328 | Val Loss: 0.9130 Acc: 0.6830\n",
      "Epoch [12/20] Train Loss: 1.0010 Acc: 0.6361 | Val Loss: 0.8909 Acc: 0.6925\n",
      "Epoch [13/20] Train Loss: 0.9742 Acc: 0.6488 | Val Loss: 0.9042 Acc: 0.6911\n",
      "Epoch [14/20] Train Loss: 0.9591 Acc: 0.6533 | Val Loss: 0.8702 Acc: 0.6980\n",
      "Epoch [15/20] Train Loss: 0.9368 Acc: 0.6625 | Val Loss: 0.8745 Acc: 0.6959\n",
      "Epoch [16/20] Train Loss: 0.9210 Acc: 0.6658 | Val Loss: 0.9157 Acc: 0.6820\n",
      "Epoch [17/20] Train Loss: 0.9063 Acc: 0.6688 | Val Loss: 0.8540 Acc: 0.7018\n",
      "Epoch [18/20] Train Loss: 0.8884 Acc: 0.6751 | Val Loss: 0.8601 Acc: 0.7026\n",
      "Epoch [19/20] Train Loss: 0.8696 Acc: 0.6807 | Val Loss: 0.8728 Acc: 0.6943\n",
      "Epoch [20/20] Train Loss: 0.8543 Acc: 0.6867 | Val Loss: 0.8579 Acc: 0.7016\n",
      "Run 3 completed in 103.82 seconds. Final Test Accuracy: 0.7016\n",
      "\n",
      "--- CIFAR10 Final Results (3 Runs) (PyTorch) ---\n",
      "Individual Test Accuracies: ['0.7068', '0.6849', '0.7016']\n",
      "Average Test Accuracy: 0.6978\n",
      "Standard Deviation of Test Accuracy: 0.0093\n",
      "Average Run Time: 105.30 seconds\n",
      "Total Time for CIFAR10: 378.29 seconds\n",
      "Accuracy plot saved to: plots_pytorch/results_cifar10_accuracy_plot_pytorch.png\n",
      "Loss plot saved to: plots_pytorch/results_cifar10_loss_log_plot_pytorch.png\n",
      "\n",
      "=== Additional Classification Metrics (sklearn) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7252    0.7760    0.7498      1000\n",
      "           1     0.8629    0.7550    0.8053      1000\n",
      "           2     0.6186    0.5450    0.5795      1000\n",
      "           3     0.4793    0.5680    0.5199      1000\n",
      "           4     0.6242    0.6710    0.6467      1000\n",
      "           5     0.6264    0.5500    0.5857      1000\n",
      "           6     0.7220    0.8130    0.7648      1000\n",
      "           7     0.8079    0.7190    0.7608      1000\n",
      "           8     0.8259    0.8110    0.8184      1000\n",
      "           9     0.7784    0.8080    0.7929      1000\n",
      "\n",
      "    accuracy                         0.7016     10000\n",
      "   macro avg     0.7071    0.7016    0.7024     10000\n",
      "weighted avg     0.7071    0.7016    0.7024     10000\n",
      "\n",
      "\n",
      "--- Processing Dataset: CIFAR100 (PyTorch) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:17<00:00, 9.86MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: cifar100 with 50000 training samples and 10000 test samples.\n",
      "\n",
      "--- Run 1/3 ---\n",
      "Epoch [1/20] Train Loss: 4.3862 Acc: 0.0352 | Val Loss: 3.9690 Acc: 0.1003\n",
      "Epoch [2/20] Train Loss: 4.0742 Acc: 0.0651 | Val Loss: 3.7945 Acc: 0.1342\n",
      "Epoch [3/20] Train Loss: 3.9537 Acc: 0.0781 | Val Loss: 3.6501 Acc: 0.1625\n",
      "Epoch [4/20] Train Loss: 3.8973 Acc: 0.0868 | Val Loss: 3.5931 Acc: 0.1744\n",
      "Epoch [5/20] Train Loss: 3.8453 Acc: 0.0922 | Val Loss: 3.5193 Acc: 0.1895\n",
      "Epoch [6/20] Train Loss: 3.8185 Acc: 0.0937 | Val Loss: 3.4550 Acc: 0.1968\n",
      "Epoch [7/20] Train Loss: 3.7884 Acc: 0.0969 | Val Loss: 3.4643 Acc: 0.1927\n",
      "Epoch [8/20] Train Loss: 3.7644 Acc: 0.1019 | Val Loss: 3.4469 Acc: 0.1998\n",
      "Epoch [9/20] Train Loss: 3.7440 Acc: 0.1049 | Val Loss: 3.4307 Acc: 0.2137\n",
      "Epoch [10/20] Train Loss: 3.7190 Acc: 0.1060 | Val Loss: 3.3915 Acc: 0.2128\n",
      "Epoch [11/20] Train Loss: 3.7012 Acc: 0.1072 | Val Loss: 3.3336 Acc: 0.2183\n",
      "Epoch [12/20] Train Loss: 3.6784 Acc: 0.1126 | Val Loss: 3.2887 Acc: 0.2232\n",
      "Epoch [13/20] Train Loss: 3.6596 Acc: 0.1134 | Val Loss: 3.3049 Acc: 0.2296\n",
      "Epoch [14/20] Train Loss: 3.6466 Acc: 0.1122 | Val Loss: 3.2965 Acc: 0.2356\n",
      "Epoch [15/20] Train Loss: 3.6331 Acc: 0.1152 | Val Loss: 3.2600 Acc: 0.2310\n",
      "Epoch [16/20] Train Loss: 3.6202 Acc: 0.1161 | Val Loss: 3.2352 Acc: 0.2351\n",
      "Epoch [17/20] Train Loss: 3.5982 Acc: 0.1215 | Val Loss: 3.1893 Acc: 0.2433\n",
      "Epoch [18/20] Train Loss: 3.5916 Acc: 0.1208 | Val Loss: 3.2236 Acc: 0.2326\n",
      "Epoch [19/20] Train Loss: 3.5774 Acc: 0.1232 | Val Loss: 3.2001 Acc: 0.2387\n",
      "Epoch [20/20] Train Loss: 3.5656 Acc: 0.1238 | Val Loss: 3.1670 Acc: 0.2510\n",
      "Run 1 completed in 116.62 seconds. Final Test Accuracy: 0.2510\n",
      "\n",
      "--- Run 2/3 ---\n",
      "Epoch [1/20] Train Loss: 4.3295 Acc: 0.0371 | Val Loss: 3.9576 Acc: 0.0999\n",
      "Epoch [2/20] Train Loss: 3.9915 Acc: 0.0751 | Val Loss: 3.7073 Acc: 0.1424\n",
      "Epoch [3/20] Train Loss: 3.8651 Acc: 0.0934 | Val Loss: 3.5634 Acc: 0.1764\n",
      "Epoch [4/20] Train Loss: 3.7768 Acc: 0.1046 | Val Loss: 3.4769 Acc: 0.1936\n",
      "Epoch [5/20] Train Loss: 3.7204 Acc: 0.1138 | Val Loss: 3.3788 Acc: 0.2096\n",
      "Epoch [6/20] Train Loss: 3.6782 Acc: 0.1216 | Val Loss: 3.4089 Acc: 0.2103\n",
      "Epoch [7/20] Train Loss: 3.6396 Acc: 0.1248 | Val Loss: 3.3038 Acc: 0.2227\n",
      "Epoch [8/20] Train Loss: 3.6007 Acc: 0.1311 | Val Loss: 3.2894 Acc: 0.2289\n",
      "Epoch [9/20] Train Loss: 3.5752 Acc: 0.1348 | Val Loss: 3.2406 Acc: 0.2367\n",
      "Epoch [10/20] Train Loss: 3.5411 Acc: 0.1391 | Val Loss: 3.1928 Acc: 0.2446\n",
      "Epoch [11/20] Train Loss: 3.5159 Acc: 0.1427 | Val Loss: 3.2039 Acc: 0.2419\n",
      "Epoch [12/20] Train Loss: 3.4955 Acc: 0.1450 | Val Loss: 3.1845 Acc: 0.2508\n",
      "Epoch [13/20] Train Loss: 3.4689 Acc: 0.1496 | Val Loss: 3.1222 Acc: 0.2633\n",
      "Epoch [14/20] Train Loss: 3.4434 Acc: 0.1546 | Val Loss: 3.0647 Acc: 0.2652\n",
      "Epoch [15/20] Train Loss: 3.4243 Acc: 0.1561 | Val Loss: 3.0439 Acc: 0.2737\n",
      "Epoch [16/20] Train Loss: 3.3997 Acc: 0.1610 | Val Loss: 3.0829 Acc: 0.2696\n",
      "Epoch [17/20] Train Loss: 3.3861 Acc: 0.1626 | Val Loss: 3.0630 Acc: 0.2656\n",
      "Epoch [18/20] Train Loss: 3.3664 Acc: 0.1651 | Val Loss: 3.0178 Acc: 0.2740\n",
      "Epoch [19/20] Train Loss: 3.3558 Acc: 0.1654 | Val Loss: 3.0175 Acc: 0.2769\n",
      "Epoch [20/20] Train Loss: 3.3364 Acc: 0.1696 | Val Loss: 2.9823 Acc: 0.2774\n",
      "Run 2 completed in 152.68 seconds. Final Test Accuracy: 0.2774\n",
      "\n",
      "--- Run 3/3 ---\n",
      "Epoch [1/20] Train Loss: 4.3371 Acc: 0.0385 | Val Loss: 3.9474 Acc: 0.1001\n",
      "Epoch [2/20] Train Loss: 3.9793 Acc: 0.0781 | Val Loss: 3.6743 Acc: 0.1463\n",
      "Epoch [3/20] Train Loss: 3.8395 Acc: 0.1006 | Val Loss: 3.5610 Acc: 0.1707\n",
      "Epoch [4/20] Train Loss: 3.7482 Acc: 0.1102 | Val Loss: 3.4818 Acc: 0.1861\n",
      "Epoch [5/20] Train Loss: 3.6853 Acc: 0.1216 | Val Loss: 3.4023 Acc: 0.2031\n",
      "Epoch [6/20] Train Loss: 3.6422 Acc: 0.1278 | Val Loss: 3.3366 Acc: 0.2212\n",
      "Epoch [7/20] Train Loss: 3.5982 Acc: 0.1343 | Val Loss: 3.2935 Acc: 0.2228\n",
      "Epoch [8/20] Train Loss: 3.5626 Acc: 0.1355 | Val Loss: 3.2291 Acc: 0.2376\n",
      "Epoch [9/20] Train Loss: 3.5367 Acc: 0.1418 | Val Loss: 3.2263 Acc: 0.2408\n",
      "Epoch [10/20] Train Loss: 3.5063 Acc: 0.1465 | Val Loss: 3.1897 Acc: 0.2451\n",
      "Epoch [11/20] Train Loss: 3.4756 Acc: 0.1490 | Val Loss: 3.1525 Acc: 0.2507\n",
      "Epoch [12/20] Train Loss: 3.4588 Acc: 0.1542 | Val Loss: 3.1546 Acc: 0.2543\n",
      "Epoch [13/20] Train Loss: 3.4414 Acc: 0.1567 | Val Loss: 3.0891 Acc: 0.2685\n",
      "Epoch [14/20] Train Loss: 3.4199 Acc: 0.1609 | Val Loss: 3.0862 Acc: 0.2610\n",
      "Epoch [15/20] Train Loss: 3.4059 Acc: 0.1622 | Val Loss: 3.0803 Acc: 0.2623\n",
      "Epoch [16/20] Train Loss: 3.3952 Acc: 0.1646 | Val Loss: 3.0527 Acc: 0.2680\n",
      "Epoch [17/20] Train Loss: 3.3740 Acc: 0.1657 | Val Loss: 3.0711 Acc: 0.2694\n",
      "Epoch [18/20] Train Loss: 3.3510 Acc: 0.1685 | Val Loss: 3.0206 Acc: 0.2687\n",
      "Epoch [19/20] Train Loss: 3.3389 Acc: 0.1734 | Val Loss: 2.9838 Acc: 0.2781\n",
      "Epoch [20/20] Train Loss: 3.3240 Acc: 0.1717 | Val Loss: 2.9973 Acc: 0.2810\n",
      "Run 3 completed in 147.59 seconds. Final Test Accuracy: 0.2810\n",
      "\n",
      "--- CIFAR100 Final Results (3 Runs) (PyTorch) ---\n",
      "Individual Test Accuracies: ['0.2510', '0.2774', '0.2810']\n",
      "Average Test Accuracy: 0.2698\n",
      "Standard Deviation of Test Accuracy: 0.0134\n",
      "Average Run Time: 138.96 seconds\n",
      "Total Time for CIFAR100: 438.97 seconds\n",
      "Accuracy plot saved to: plots_pytorch/results_cifar100_accuracy_plot_pytorch.png\n",
      "Loss plot saved to: plots_pytorch/results_cifar100_loss_log_plot_pytorch.png\n",
      "\n",
      "=== Additional Classification Metrics (sklearn) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5093    0.5500    0.5288       100\n",
      "           1     0.3067    0.2300    0.2629       100\n",
      "           2     0.2078    0.1600    0.1808       100\n",
      "           3     0.2212    0.2300    0.2255       100\n",
      "           4     0.1579    0.0900    0.1146       100\n",
      "           5     0.2658    0.2100    0.2346       100\n",
      "           6     0.3187    0.2900    0.3037       100\n",
      "           7     0.3293    0.2700    0.2967       100\n",
      "           8     0.2551    0.5000    0.3378       100\n",
      "           9     0.3558    0.3700    0.3627       100\n",
      "          10     0.2400    0.1200    0.1600       100\n",
      "          11     0.2407    0.1300    0.1688       100\n",
      "          12     0.2596    0.2700    0.2647       100\n",
      "          13     0.2549    0.2600    0.2574       100\n",
      "          14     0.1579    0.1500    0.1538       100\n",
      "          15     0.1081    0.0400    0.0584       100\n",
      "          16     0.4231    0.2200    0.2895       100\n",
      "          17     0.3889    0.5600    0.4590       100\n",
      "          18     0.2626    0.2600    0.2613       100\n",
      "          19     0.2069    0.1200    0.1519       100\n",
      "          20     0.4737    0.6300    0.5408       100\n",
      "          21     0.3727    0.4100    0.3905       100\n",
      "          22     0.3400    0.1700    0.2267       100\n",
      "          23     0.4286    0.5700    0.4893       100\n",
      "          24     0.3177    0.6100    0.4178       100\n",
      "          25     0.2540    0.1600    0.1963       100\n",
      "          26     0.1407    0.1900    0.1617       100\n",
      "          27     0.1676    0.3000    0.2151       100\n",
      "          28     0.3978    0.3700    0.3834       100\n",
      "          29     0.2674    0.2300    0.2473       100\n",
      "          30     0.2281    0.2600    0.2430       100\n",
      "          31     0.2857    0.3000    0.2927       100\n",
      "          32     0.4857    0.1700    0.2519       100\n",
      "          33     0.2671    0.3900    0.3171       100\n",
      "          34     0.1717    0.1700    0.1709       100\n",
      "          35     0.1917    0.2300    0.2091       100\n",
      "          36     0.2843    0.2900    0.2871       100\n",
      "          37     0.2614    0.2300    0.2447       100\n",
      "          38     0.1739    0.0800    0.1096       100\n",
      "          39     0.1429    0.0700    0.0940       100\n",
      "          40     0.3448    0.1000    0.1550       100\n",
      "          41     0.7966    0.4700    0.5912       100\n",
      "          42     0.1485    0.3400    0.2067       100\n",
      "          43     0.2232    0.2500    0.2358       100\n",
      "          44     0.1019    0.1100    0.1058       100\n",
      "          45     0.1250    0.0900    0.1047       100\n",
      "          46     0.2558    0.1100    0.1538       100\n",
      "          47     0.4923    0.3200    0.3879       100\n",
      "          48     0.3757    0.7100    0.4913       100\n",
      "          49     0.3768    0.2600    0.3077       100\n",
      "          50     0.1379    0.0400    0.0620       100\n",
      "          51     0.1081    0.0800    0.0920       100\n",
      "          52     0.4318    0.7600    0.5507       100\n",
      "          53     0.4435    0.5100    0.4744       100\n",
      "          54     0.3837    0.3300    0.3548       100\n",
      "          55     0.0674    0.0600    0.0635       100\n",
      "          56     0.3298    0.3100    0.3196       100\n",
      "          57     0.4130    0.1900    0.2603       100\n",
      "          58     0.3303    0.3600    0.3445       100\n",
      "          59     0.1731    0.0900    0.1184       100\n",
      "          60     0.5776    0.6700    0.6204       100\n",
      "          61     0.4300    0.4300    0.4300       100\n",
      "          62     0.3353    0.5700    0.4222       100\n",
      "          63     0.2074    0.4500    0.2839       100\n",
      "          64     0.0278    0.0100    0.0147       100\n",
      "          65     0.0345    0.0100    0.0155       100\n",
      "          66     0.0972    0.1400    0.1148       100\n",
      "          67     0.2143    0.1200    0.1538       100\n",
      "          68     0.6136    0.5400    0.5745       100\n",
      "          69     0.5424    0.3200    0.4025       100\n",
      "          70     0.2590    0.4300    0.3233       100\n",
      "          71     0.4951    0.5100    0.5025       100\n",
      "          72     0.0667    0.0100    0.0174       100\n",
      "          73     0.2416    0.3600    0.2892       100\n",
      "          74     0.0826    0.1900    0.1152       100\n",
      "          75     0.3587    0.6600    0.4648       100\n",
      "          76     0.5679    0.4600    0.5083       100\n",
      "          77     0.0750    0.0300    0.0429       100\n",
      "          78     0.0625    0.0300    0.0405       100\n",
      "          79     0.3200    0.0800    0.1280       100\n",
      "          80     0.1395    0.0600    0.0839       100\n",
      "          81     0.2404    0.2500    0.2451       100\n",
      "          82     0.5564    0.7400    0.6352       100\n",
      "          83     0.1944    0.1400    0.1628       100\n",
      "          84     0.1449    0.1000    0.1183       100\n",
      "          85     0.4040    0.4000    0.4020       100\n",
      "          86     0.2697    0.4800    0.3453       100\n",
      "          87     0.2815    0.3800    0.3234       100\n",
      "          88     0.1245    0.3200    0.1793       100\n",
      "          89     0.2628    0.4100    0.3203       100\n",
      "          90     0.2895    0.1100    0.1594       100\n",
      "          91     0.2865    0.5500    0.3767       100\n",
      "          92     0.0714    0.0200    0.0312       100\n",
      "          93     0.0938    0.0300    0.0455       100\n",
      "          94     0.5051    0.5000    0.5025       100\n",
      "          95     0.2955    0.5200    0.3768       100\n",
      "          96     0.2264    0.1200    0.1569       100\n",
      "          97     0.1402    0.3700    0.2033       100\n",
      "          98     0.1733    0.1300    0.1486       100\n",
      "          99     0.2000    0.1000    0.1333       100\n",
      "\n",
      "    accuracy                         0.2810     10000\n",
      "   macro avg     0.2769    0.2810    0.2636     10000\n",
      "weighted avg     0.2769    0.2810    0.2636     10000\n",
      "\n",
      "\n",
      "--- Processing Dataset: OXFORD_IIIT_PET (PyTorch) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792M/792M [00:32<00:00, 24.6MB/s] \n",
      "100%|██████████| 19.2M/19.2M [00:01<00:00, 11.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: oxford_iiit_pet with 3680 training samples and 3669 test samples.\n",
      "\n",
      "--- Run 1/3 ---\n",
      "Epoch [1/20] Train Loss: 3.6806 Acc: 0.0272 | Val Loss: 3.6140 Acc: 0.0273\n",
      "Epoch [2/20] Train Loss: 3.6137 Acc: 0.0223 | Val Loss: 3.6136 Acc: 0.0273\n",
      "Epoch [3/20] Train Loss: 3.6101 Acc: 0.0288 | Val Loss: 3.5978 Acc: 0.0401\n",
      "Epoch [4/20] Train Loss: 3.5973 Acc: 0.0378 | Val Loss: 3.5824 Acc: 0.0523\n",
      "Epoch [5/20] Train Loss: 3.5760 Acc: 0.0418 | Val Loss: 3.5547 Acc: 0.0548\n",
      "Epoch [6/20] Train Loss: 3.5621 Acc: 0.0408 | Val Loss: 3.5402 Acc: 0.0537\n",
      "Epoch [7/20] Train Loss: 3.5606 Acc: 0.0424 | Val Loss: 3.5241 Acc: 0.0526\n",
      "Epoch [8/20] Train Loss: 3.5447 Acc: 0.0418 | Val Loss: 3.5082 Acc: 0.0561\n",
      "Epoch [9/20] Train Loss: 3.5348 Acc: 0.0394 | Val Loss: 3.5041 Acc: 0.0624\n",
      "Epoch [10/20] Train Loss: 3.5110 Acc: 0.0437 | Val Loss: 3.4600 Acc: 0.0597\n",
      "Epoch [11/20] Train Loss: 3.4910 Acc: 0.0489 | Val Loss: 3.4577 Acc: 0.0725\n",
      "Epoch [12/20] Train Loss: 3.4953 Acc: 0.0459 | Val Loss: 3.4515 Acc: 0.0630\n",
      "Epoch [13/20] Train Loss: 3.4836 Acc: 0.0481 | Val Loss: 3.4473 Acc: 0.0733\n",
      "Epoch [14/20] Train Loss: 3.4823 Acc: 0.0516 | Val Loss: 3.4166 Acc: 0.0725\n",
      "Epoch [15/20] Train Loss: 3.4546 Acc: 0.0516 | Val Loss: 3.4340 Acc: 0.0717\n",
      "Epoch [16/20] Train Loss: 3.4505 Acc: 0.0533 | Val Loss: 3.4204 Acc: 0.0706\n",
      "Epoch [17/20] Train Loss: 3.4231 Acc: 0.0590 | Val Loss: 3.3930 Acc: 0.0812\n",
      "Epoch [18/20] Train Loss: 3.4030 Acc: 0.0641 | Val Loss: 3.3889 Acc: 0.0804\n",
      "Epoch [19/20] Train Loss: 3.3937 Acc: 0.0649 | Val Loss: 3.3801 Acc: 0.0771\n",
      "Epoch [20/20] Train Loss: 3.3758 Acc: 0.0715 | Val Loss: 3.3706 Acc: 0.0968\n",
      "Run 1 completed in 290.37 seconds. Final Test Accuracy: 0.0968\n",
      "\n",
      "--- Run 2/3 ---\n",
      "Epoch [1/20] Train Loss: 3.6656 Acc: 0.0293 | Val Loss: 3.6077 Acc: 0.0333\n",
      "Epoch [2/20] Train Loss: 3.5964 Acc: 0.0307 | Val Loss: 3.5611 Acc: 0.0425\n",
      "Epoch [3/20] Train Loss: 3.5530 Acc: 0.0397 | Val Loss: 3.5271 Acc: 0.0515\n",
      "Epoch [4/20] Train Loss: 3.5315 Acc: 0.0503 | Val Loss: 3.4956 Acc: 0.0493\n",
      "Epoch [5/20] Train Loss: 3.4958 Acc: 0.0516 | Val Loss: 3.4480 Acc: 0.0613\n",
      "Epoch [6/20] Train Loss: 3.4709 Acc: 0.0576 | Val Loss: 3.4365 Acc: 0.0660\n",
      "Epoch [7/20] Train Loss: 3.4524 Acc: 0.0601 | Val Loss: 3.4082 Acc: 0.0695\n",
      "Epoch [8/20] Train Loss: 3.4341 Acc: 0.0595 | Val Loss: 3.3865 Acc: 0.0698\n",
      "Epoch [9/20] Train Loss: 3.4086 Acc: 0.0582 | Val Loss: 3.3836 Acc: 0.0763\n",
      "Epoch [10/20] Train Loss: 3.3826 Acc: 0.0712 | Val Loss: 3.3415 Acc: 0.0891\n",
      "Epoch [11/20] Train Loss: 3.3500 Acc: 0.0704 | Val Loss: 3.3557 Acc: 0.0853\n",
      "Epoch [12/20] Train Loss: 3.3368 Acc: 0.0682 | Val Loss: 3.3573 Acc: 0.0965\n",
      "Epoch [13/20] Train Loss: 3.3201 Acc: 0.0712 | Val Loss: 3.3302 Acc: 0.0987\n",
      "Epoch [14/20] Train Loss: 3.2960 Acc: 0.0804 | Val Loss: 3.3067 Acc: 0.1038\n",
      "Epoch [15/20] Train Loss: 3.2504 Acc: 0.0916 | Val Loss: 3.3195 Acc: 0.0970\n",
      "Epoch [16/20] Train Loss: 3.2083 Acc: 0.0864 | Val Loss: 3.2774 Acc: 0.1063\n",
      "Epoch [17/20] Train Loss: 3.1604 Acc: 0.1049 | Val Loss: 3.2749 Acc: 0.1049\n",
      "Epoch [18/20] Train Loss: 3.1342 Acc: 0.1092 | Val Loss: 3.2805 Acc: 0.1142\n",
      "Epoch [19/20] Train Loss: 3.0796 Acc: 0.1185 | Val Loss: 3.2542 Acc: 0.1090\n",
      "Epoch [20/20] Train Loss: 3.0333 Acc: 0.1174 | Val Loss: 3.2609 Acc: 0.1055\n",
      "Run 2 completed in 260.64 seconds. Final Test Accuracy: 0.1055\n",
      "\n",
      "--- Run 3/3 ---\n",
      "Epoch [1/20] Train Loss: 3.6433 Acc: 0.0288 | Val Loss: 3.6028 Acc: 0.0447\n",
      "Epoch [2/20] Train Loss: 3.5694 Acc: 0.0421 | Val Loss: 3.5445 Acc: 0.0493\n",
      "Epoch [3/20] Train Loss: 3.5243 Acc: 0.0505 | Val Loss: 3.5141 Acc: 0.0469\n",
      "Epoch [4/20] Train Loss: 3.5021 Acc: 0.0573 | Val Loss: 3.4873 Acc: 0.0597\n",
      "Epoch [5/20] Train Loss: 3.4645 Acc: 0.0595 | Val Loss: 3.4697 Acc: 0.0692\n",
      "Epoch [6/20] Train Loss: 3.4475 Acc: 0.0633 | Val Loss: 3.4543 Acc: 0.0602\n",
      "Epoch [7/20] Train Loss: 3.4118 Acc: 0.0690 | Val Loss: 3.4317 Acc: 0.0657\n",
      "Epoch [8/20] Train Loss: 3.3670 Acc: 0.0769 | Val Loss: 3.4071 Acc: 0.0720\n",
      "Epoch [9/20] Train Loss: 3.3060 Acc: 0.0957 | Val Loss: 3.3729 Acc: 0.0837\n",
      "Epoch [10/20] Train Loss: 3.2582 Acc: 0.0965 | Val Loss: 3.3748 Acc: 0.0957\n",
      "Epoch [11/20] Train Loss: 3.1620 Acc: 0.1168 | Val Loss: 3.3223 Acc: 0.0987\n",
      "Epoch [12/20] Train Loss: 3.0747 Acc: 0.1326 | Val Loss: 3.3194 Acc: 0.1036\n",
      "Epoch [13/20] Train Loss: 3.0138 Acc: 0.1495 | Val Loss: 3.3292 Acc: 0.1077\n",
      "Epoch [14/20] Train Loss: 2.9504 Acc: 0.1617 | Val Loss: 3.3211 Acc: 0.1019\n",
      "Epoch [15/20] Train Loss: 2.8520 Acc: 0.1791 | Val Loss: 3.3185 Acc: 0.1079\n",
      "Epoch [16/20] Train Loss: 2.7953 Acc: 0.1910 | Val Loss: 3.3312 Acc: 0.1112\n",
      "Epoch [17/20] Train Loss: 2.7008 Acc: 0.2065 | Val Loss: 3.3740 Acc: 0.1123\n",
      "Epoch [18/20] Train Loss: 2.6509 Acc: 0.2101 | Val Loss: 3.3938 Acc: 0.1128\n",
      "Epoch [19/20] Train Loss: 2.5935 Acc: 0.2269 | Val Loss: 3.4018 Acc: 0.1115\n",
      "Epoch [20/20] Train Loss: 2.5416 Acc: 0.2304 | Val Loss: 3.4991 Acc: 0.1047\n",
      "Run 3 completed in 256.66 seconds. Final Test Accuracy: 0.1047\n",
      "\n",
      "--- OXFORD_IIIT_PET Final Results (3 Runs) (PyTorch) ---\n",
      "Individual Test Accuracies: ['0.0968', '0.1055', '0.1047']\n",
      "Average Test Accuracy: 0.1023\n",
      "Standard Deviation of Test Accuracy: 0.0039\n",
      "Average Run Time: 269.22 seconds\n",
      "Total Time for OXFORD_IIIT_PET: 859.52 seconds\n",
      "Accuracy plot saved to: plots_pytorch/results_oxford_iiit_pet_accuracy_plot_pytorch.png\n",
      "Loss plot saved to: plots_pytorch/results_oxford_iiit_pet_loss_log_plot_pytorch.png\n",
      "\n",
      "=== Additional Classification Metrics (sklearn) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0994    0.1633    0.1236        98\n",
      "           1     0.0862    0.0500    0.0633       100\n",
      "           2     0.0000    0.0000    0.0000       100\n",
      "           3     0.0986    0.0700    0.0819       100\n",
      "           4     0.1028    0.1100    0.1063       100\n",
      "           5     0.1573    0.1400    0.1481       100\n",
      "           6     0.1774    0.1100    0.1358       100\n",
      "           7     0.2083    0.2841    0.2404        88\n",
      "           8     0.0680    0.0707    0.0693        99\n",
      "           9     0.0476    0.0200    0.0282       100\n",
      "          10     0.0390    0.0300    0.0339       100\n",
      "          11     0.1429    0.0515    0.0758        97\n",
      "          12     0.0137    0.0100    0.0116       100\n",
      "          13     0.0556    0.0400    0.0465       100\n",
      "          14     0.0805    0.0700    0.0749       100\n",
      "          15     0.2045    0.0900    0.1250       100\n",
      "          16     0.0289    0.0500    0.0366       100\n",
      "          17     0.1460    0.2000    0.1688       100\n",
      "          18     0.0824    0.0707    0.0761        99\n",
      "          19     0.1184    0.2700    0.1646       100\n",
      "          20     0.0725    0.0500    0.0592       100\n",
      "          21     0.1205    0.1000    0.1093       100\n",
      "          22     0.1461    0.2600    0.1871       100\n",
      "          23     0.1278    0.2900    0.1774       100\n",
      "          24     0.0682    0.0900    0.0776       100\n",
      "          25     0.0909    0.0800    0.0851       100\n",
      "          26     0.1243    0.2200    0.1588       100\n",
      "          27     0.1940    0.1300    0.1557       100\n",
      "          28     0.0796    0.0900    0.0845       100\n",
      "          29     0.1329    0.1900    0.1564       100\n",
      "          30     0.1780    0.2121    0.1935        99\n",
      "          31     0.0448    0.0300    0.0359       100\n",
      "          32     0.1556    0.0700    0.0966       100\n",
      "          33     0.1136    0.0500    0.0694       100\n",
      "          34     0.0702    0.0899    0.0788        89\n",
      "          35     0.0143    0.0100    0.0118       100\n",
      "          36     0.0366    0.0300    0.0330       100\n",
      "\n",
      "    accuracy                         0.1047      3669\n",
      "   macro avg     0.1007    0.1052    0.0968      3669\n",
      "weighted avg     0.1004    0.1047    0.0963      3669\n",
      "\n",
      "\n",
      "--- All Dataset Processing Complete (PyTorch) ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import classification_report  # For additional classification metrics\n",
    "\n",
    "# --- Configuration & Hyperparameters ---\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "DROPOUT_RATE = 0.5\n",
    "ADAM_BETA_1 = 0.9\n",
    "ADAM_BETA_2 = 0.999\n",
    "ADAM_EPSILON = 1e-7 # Note: PyTorch Adam uses 'eps' not 'epsilon'\n",
    "NUM_RUNS = 3\n",
    "PLOT_SAVE_DIR = \"plots_pytorch\"  # Directory to save the plots\n",
    "\n",
    "# --- Device Selection ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Dataset Configurations (Adapted for PyTorch/Torchvision) ---\n",
    "DATASETS = {\n",
    "    \"mnist\": {\n",
    "        \"name\": \"mnist\",\n",
    "        \"load_fn\": torchvision.datasets.MNIST,\n",
    "        \"input_shape\": (1, 28, 28), # PyTorch: C, H, W\n",
    "        \"num_classes\": 10,\n",
    "        \"target_transform\": None # Labels are already integers\n",
    "    },\n",
    "    \"fashion_mnist\": {\n",
    "        \"name\": \"fashion_mnist\",\n",
    "        \"load_fn\": torchvision.datasets.FashionMNIST,\n",
    "        \"input_shape\": (1, 28, 28),\n",
    "        \"num_classes\": 10,\n",
    "         \"target_transform\": None\n",
    "    },\n",
    "    \"cifar10\": {\n",
    "        \"name\": \"cifar10\",\n",
    "        \"load_fn\": torchvision.datasets.CIFAR10,\n",
    "        \"input_shape\": (3, 32, 32),\n",
    "        \"num_classes\": 10,\n",
    "         \"target_transform\": None\n",
    "    },\n",
    "    \"cifar100\": {\n",
    "        \"name\": \"cifar100\",\n",
    "        \"load_fn\": torchvision.datasets.CIFAR100,\n",
    "        \"input_shape\": (3, 32, 32),\n",
    "        \"num_classes\": 100,\n",
    "         \"target_transform\": None\n",
    "    },\n",
    "    \"oxford_iiit_pet\": {\n",
    "        \"name\": \"oxford_iiit_pet\",\n",
    "        \"load_fn\": torchvision.datasets.OxfordIIITPet,\n",
    "        \"input_shape\": (3, 128, 128), # Resize target\n",
    "        \"num_classes\": 37,\n",
    "        # Labels are 0-36, already suitable for CrossEntropyLoss\n",
    "        \"target_transform\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Preprocessing Function (using torchvision.transforms) ---\n",
    "def get_transforms(input_shape):\n",
    "    \"\"\"Returns appropriate transforms for training and testing.\"\"\"\n",
    "    # Basic transforms: Resize, convert to tensor, normalize to [0, 1]\n",
    "    # PyTorch ToTensor automatically scales to [0, 1] and permutes to C, H, W\n",
    "    img_size = input_shape[1:] # H, W\n",
    "    num_channels = input_shape[0] # C\n",
    "\n",
    "    transform_list = [transforms.Resize(img_size)]\n",
    "\n",
    "    # Handle grayscale conversion if needed (input is RGB but model needs Gray)\n",
    "    if num_channels == 1:\n",
    "         # Add grayscale transform *if* the source dataset isn't already grayscale\n",
    "         # MNIST/FashionMNIST are loaded as grayscale by default.\n",
    "         # For OxfordPet, if we wanted grayscale, we'd add Grayscale here.\n",
    "         # transform_list.append(transforms.Grayscale(num_output_channels=1))\n",
    "         pass # Assuming dataset loader provides correct number of channels\n",
    "\n",
    "    # Handle RGB conversion if needed (input is Gray but model needs RGB)\n",
    "    elif num_channels == 3:\n",
    "         # Add RGB conversion *if* the source dataset isn't already RGB\n",
    "         # MNIST/FashionMNIST would need this if target shape was 3 channels.\n",
    "         # transform_list.append(transforms.Grayscale(num_output_channels=3)) # Hacky way\n",
    "         pass # Assuming dataset loader provides correct number of channels\n",
    "\n",
    "\n",
    "    transform_list.append(transforms.ToTensor()) # Converts to Tensor, scales to [0,1], changes to C, H, W\n",
    "\n",
    "    # Optional: Add normalization (e.g., for CIFAR)\n",
    "    # if input_shape == (3, 32, 32): # Example for CIFAR\n",
    "    #     transform_list.append(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
    "\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "# --- Model Building Function (PyTorch nn.Module) ---\n",
    "class ClassifierCNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, dropout_rate):\n",
    "        super(ClassifierCNN, self).__init__()\n",
    "        C, H, W = input_shape\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=C, out_channels=32, kernel_size=3, padding='same'), # Use padding='same' for simplicity\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # Calculate shape after conv1 + pool1\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # Calculate shape after conv2 + pool2\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "\n",
    "        # Calculate the flattened size\n",
    "        flattened_size = 64 * H * W\n",
    "\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=flattened_size, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # Output raw logits for nn.CrossEntropyLoss\n",
    "            nn.Linear(in_features=64, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.fc_block(x)\n",
    "        return x\n",
    "\n",
    "# --- Plotting Functions (Adapted for list-based history) ---\n",
    "def plot_mean_std_curves(histories, dataset_name, epochs, save_dir=\"plots_pytorch\"):\n",
    "    \"\"\"Plots mean and std dev of training and validation accuracy and saves the figure.\"\"\"\n",
    "    # histories is expected to be a list of dictionaries, where each dict contains\n",
    "    # 'accuracy', 'val_accuracy', 'loss', 'val_loss' lists.\n",
    "\n",
    "    all_acc = [h['accuracy'] for h in histories]\n",
    "    all_val_acc = [h['val_accuracy'] for h in histories]\n",
    "\n",
    "    # Pad shorter histories if runs ended early (though not expected with fixed epochs)\n",
    "    max_len = epochs # Assume all runs completed all epochs\n",
    "    all_acc = [np.pad(acc, (0, max_len - len(acc)), 'edge') for acc in all_acc]\n",
    "    all_val_acc = [np.pad(val_acc, (0, max_len - len(val_acc)), 'edge') for val_acc in all_val_acc]\n",
    "\n",
    "    mean_acc = np.mean(all_acc, axis=0)\n",
    "    std_acc = np.std(all_acc, axis=0)\n",
    "    mean_val_acc = np.mean(all_val_acc, axis=0)\n",
    "    std_val_acc = np.std(all_val_acc, axis=0)\n",
    "\n",
    "    epoch_range = range(1, max_len + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(f'{dataset_name.upper()} - Mean Accuracy over {len(histories)} Runs (PyTorch)')\n",
    "\n",
    "    plt.plot(epoch_range, mean_acc, label='Mean Training Accuracy', color='blue')\n",
    "    plt.fill_between(epoch_range, mean_acc - std_acc, mean_acc + std_acc,\n",
    "                     alpha=0.2, color='blue', label='Training Acc ±1 std dev')\n",
    "    plt.plot(epoch_range, mean_val_acc, label='Mean Validation Accuracy', color='orange')\n",
    "    plt.fill_between(epoch_range, mean_val_acc - std_val_acc, mean_val_acc + std_val_acc,\n",
    "                     alpha=0.2, color='orange', label='Validation Acc ±1 std dev')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = os.path.join(save_dir, f\"results_{dataset_name}_accuracy_plot_pytorch.png\")\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Accuracy plot saved to: {filename}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error saving plot to {save_dir}: {e}\")\n",
    "    plt.close() # Close the figure to free memory\n",
    "\n",
    "def plot_loss_curves_log(history, dataset_name, save_dir=\"plots_pytorch\"):\n",
    "    \"\"\"Plots training and validation loss curves from the last run.\"\"\"\n",
    "    epoch_range = range(1, len(history['loss']) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(f'{dataset_name.upper()} - Loss Curves (Last Run) (PyTorch)')\n",
    "    plt.plot(epoch_range, history['loss'], label='Training Loss', color='red')\n",
    "    plt.plot(epoch_range, history['val_loss'], label='Validation Loss', color='green')\n",
    "    # plt.yscale('log') # Optional: log scale\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    try:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = os.path.join(save_dir, f\"results_{dataset_name}_loss_log_plot_pytorch.png\")\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Loss plot saved to: {filename}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error saving plot to {save_dir}: {e}\")\n",
    "    plt.close() # Close the figure\n",
    "\n",
    "# --- Training Loop Function ---\n",
    "def train_on_dataset(dataset_key, save_plot_dir):\n",
    "    \"\"\"Loads, preprocesses, trains, and evaluates the model on a given dataset using PyTorch.\"\"\"\n",
    "    print(f\"\\n--- Processing Dataset: {dataset_key.upper()} (PyTorch) ---\")\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    config = DATASETS[dataset_key]\n",
    "    input_shape = config[\"input_shape\"]\n",
    "    num_classes = config[\"num_classes\"]\n",
    "    DatasetClass = config[\"load_fn\"]\n",
    "    target_transform = config[\"target_transform\"]\n",
    "\n",
    "    # --- Data Loading and Preprocessing ---\n",
    "    transform = get_transforms(input_shape)\n",
    "\n",
    "    # Special handling for OxfordIIITPet split argument\n",
    "    if dataset_key == \"oxford_iiit_pet\":\n",
    "        train_dataset = DatasetClass(root='./data', split='trainval', download=True, transform=transform, target_transform=target_transform)\n",
    "        test_dataset = DatasetClass(root='./data', split='test', download=True, transform=transform, target_transform=target_transform)\n",
    "    else:\n",
    "        # Most torchvision datasets use train=True/False\n",
    "        train_dataset = DatasetClass(root='./data', train=True, download=True, transform=transform, target_transform=target_transform)\n",
    "        test_dataset = DatasetClass(root='./data', train=False, download=True, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    print(f\"Loaded dataset: {config['name']} with {len(train_dataset)} training samples and {len(test_dataset)} test samples.\")\n",
    "\n",
    "    # --- Model Training over Multiple Runs ---\n",
    "    histories = []\n",
    "    test_accuracies = []\n",
    "    run_times = []\n",
    "    final_model_state = None # Store state_dict of the last model\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"\\n--- Run {run + 1}/{NUM_RUNS} ---\")\n",
    "        start_time_run = time.time()\n",
    "\n",
    "        model = ClassifierCNN(input_shape, num_classes, DROPOUT_RATE).to(DEVICE)\n",
    "        # Use CrossEntropyLoss which expects raw logits\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                               lr=LEARNING_RATE,\n",
    "                               betas=(ADAM_BETA_1, ADAM_BETA_2),\n",
    "                               eps=ADAM_EPSILON)\n",
    "\n",
    "        run_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            # --- Training Phase ---\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            epoch_acc = correct_train / total_train\n",
    "            run_history['loss'].append(epoch_loss)\n",
    "            run_history['accuracy'].append(epoch_acc)\n",
    "\n",
    "            # --- Validation Phase ---\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    running_val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_val += labels.size(0)\n",
    "                    correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_val_loss = running_val_loss / len(test_loader)\n",
    "            epoch_val_acc = correct_val / total_val\n",
    "            run_history['val_loss'].append(epoch_val_loss)\n",
    "            run_history['val_accuracy'].append(epoch_val_acc)\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "        # --- End of Run ---\n",
    "        histories.append(run_history)\n",
    "        test_accuracies.append(epoch_val_acc) # Use the final epoch's validation accuracy\n",
    "        end_time_run = time.time()\n",
    "        run_time = end_time_run - start_time_run\n",
    "        run_times.append(run_time)\n",
    "        print(f\"Run {run + 1} completed in {run_time:.2f} seconds. Final Test Accuracy: {epoch_val_acc:.4f}\")\n",
    "        if run == NUM_RUNS - 1: # Save the state of the last model\n",
    "             final_model_state = model.state_dict()\n",
    "\n",
    "\n",
    "    avg_acc = np.mean(test_accuracies)\n",
    "    std_acc = np.std(test_accuracies)\n",
    "    avg_time = np.mean(run_times)\n",
    "    total_time = time.time() - start_time_total\n",
    "\n",
    "    print(f\"\\n--- {dataset_key.upper()} Final Results ({NUM_RUNS} Runs) (PyTorch) ---\")\n",
    "    print(f\"Individual Test Accuracies: {[f'{acc:.4f}' for acc in test_accuracies]}\")\n",
    "    print(f\"Average Test Accuracy: {avg_acc:.4f}\")\n",
    "    print(f\"Standard Deviation of Test Accuracy: {std_acc:.4f}\")\n",
    "    print(f\"Average Run Time: {avg_time:.2f} seconds\")\n",
    "    print(f\"Total Time for {dataset_key.upper()}: {total_time:.2f} seconds\")\n",
    "\n",
    "    # --- Plotting Accuracy and Loss Curves ---\n",
    "    plot_mean_std_curves(histories, dataset_key, EPOCHS, save_dir=save_plot_dir)\n",
    "    plot_loss_curves_log(histories[-1], dataset_key, save_dir=save_plot_dir) # Plot last run's loss\n",
    "\n",
    "    # --- Additional Classification Metrics (using the last run's model) ---\n",
    "    if final_model_state is not None:\n",
    "        print(\"\\n=== Additional Classification Metrics (sklearn) ===\")\n",
    "        # Load the final model state\n",
    "        final_model = ClassifierCNN(input_shape, num_classes, DROPOUT_RATE).to(DEVICE)\n",
    "        final_model.load_state_dict(final_model_state)\n",
    "        final_model.eval()\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                outputs = final_model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_preds.extend(predicted.cpu().numpy()) # Move preds to CPU and convert to numpy\n",
    "                all_labels.extend(labels.cpu().numpy())   # Move labels to CPU and convert to numpy (already numpy/int if loaded correctly)\n",
    "\n",
    "        print(classification_report(all_labels, all_preds, digits=4, zero_division=0))\n",
    "        # Added zero_division=0 to handle cases where a class might have no predicted samples\n",
    "    else:\n",
    "        print(\"Warning: No final model state available for computing additional metrics.\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # GPU check is done globally via DEVICE definition\n",
    "    # Memory growth management is generally less explicit in PyTorch\n",
    "\n",
    "    try:\n",
    "        os.makedirs(PLOT_SAVE_DIR, exist_ok=True)\n",
    "        print(f\"Plots will be saved in '{PLOT_SAVE_DIR}/' directory.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Could not create plot directory '{PLOT_SAVE_DIR}': {e}. Using current directory.\")\n",
    "        PLOT_SAVE_DIR = \".\"\n",
    "\n",
    "    for dataset in DATASETS.keys():\n",
    "        train_on_dataset(dataset, save_plot_dir=PLOT_SAVE_DIR)\n",
    "\n",
    "    print(\"\\n--- All Dataset Processing Complete (PyTorch) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28d817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d1fb39a",
   "metadata": {},
   "source": [
    "# LearnableLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b64b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Plots will be saved in 'plots_learnable_loss_detailed_pytorch/' directory.\n",
      "\n",
      "==================== Processing Dataset: MNIST ====================\n",
      "Loading and preprocessing data...\n",
      "Data loading complete. Train samples: 60000, Test samples: 10000\n",
      "\n",
      "--- Starting Run 1/3 for MNIST ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.0607 | Test Acc: 97.7100% | Test Task Loss: 0.0131 | Time: 6.42s\n",
      "  Epoch 002/20 | Train Total Loss: 0.0146 | Test Acc: 98.2500% | Test Task Loss: 0.0095 | Time: 5.61s\n",
      "  Epoch 003/20 | Train Total Loss: 0.0099 | Test Acc: 98.8300% | Test Task Loss: 0.0060 | Time: 5.65s\n",
      "  Epoch 004/20 | Train Total Loss: 0.0078 | Test Acc: 99.2000% | Test Task Loss: 0.0045 | Time: 5.71s\n",
      "  Epoch 005/20 | Train Total Loss: 0.0063 | Test Acc: 98.8200% | Test Task Loss: 0.0070 | Time: 5.85s\n",
      "  Epoch 006/20 | Train Total Loss: 0.0070 | Test Acc: 99.1900% | Test Task Loss: 0.0050 | Time: 5.68s\n",
      "  Epoch 007/20 | Train Total Loss: 0.0059 | Test Acc: 98.9300% | Test Task Loss: 0.0058 | Time: 5.73s\n",
      "  Epoch 008/20 | Train Total Loss: 0.0051 | Test Acc: 99.2300% | Test Task Loss: 0.0055 | Time: 5.76s\n",
      "  Epoch 009/20 | Train Total Loss: 0.0040 | Test Acc: 99.3300% | Test Task Loss: 0.0039 | Time: 5.70s\n",
      "  Epoch 010/20 | Train Total Loss: 0.0036 | Test Acc: 99.1300% | Test Task Loss: 0.0054 | Time: 5.73s\n",
      "  Epoch 011/20 | Train Total Loss: 0.0030 | Test Acc: 99.2000% | Test Task Loss: 0.0053 | Time: 5.62s\n",
      "  Epoch 012/20 | Train Total Loss: 0.0032 | Test Acc: 99.0000% | Test Task Loss: 0.0066 | Time: 5.77s\n",
      "  Epoch 013/20 | Train Total Loss: 0.0030 | Test Acc: 99.2800% | Test Task Loss: 0.0048 | Time: 5.60s\n",
      "  Epoch 014/20 | Train Total Loss: 0.0023 | Test Acc: 99.1900% | Test Task Loss: 0.0053 | Time: 5.66s\n",
      "  Epoch 015/20 | Train Total Loss: 0.0025 | Test Acc: 99.0500% | Test Task Loss: 0.0064 | Time: 5.77s\n",
      "  Epoch 016/20 | Train Total Loss: 0.0030 | Test Acc: 99.3600% | Test Task Loss: 0.0045 | Time: 5.68s\n",
      "  Epoch 017/20 | Train Total Loss: 0.0026 | Test Acc: 99.3000% | Test Task Loss: 0.0049 | Time: 5.59s\n",
      "  Epoch 018/20 | Train Total Loss: 0.0019 | Test Acc: 98.9400% | Test Task Loss: 0.0084 | Time: 5.62s\n",
      "  Epoch 019/20 | Train Total Loss: 0.0020 | Test Acc: 99.3400% | Test Task Loss: 0.0050 | Time: 5.66s\n",
      "  Epoch 020/20 | Train Total Loss: 0.0019 | Test Acc: 99.3500% | Test Task Loss: 0.0042 | Time: 5.75s\n",
      "--- Run 1 completed in 114.56 seconds. Final Test Accuracy: 99.3500% ---\n",
      "\n",
      "--- Starting Run 2/3 for MNIST ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.0740 | Test Acc: 97.7700% | Test Task Loss: 0.0134 | Time: 5.62s\n",
      "  Epoch 002/20 | Train Total Loss: 0.0177 | Test Acc: 98.6500% | Test Task Loss: 0.0073 | Time: 5.74s\n",
      "  Epoch 003/20 | Train Total Loss: 0.0121 | Test Acc: 98.9600% | Test Task Loss: 0.0058 | Time: 6.07s\n",
      "  Epoch 004/20 | Train Total Loss: 0.0095 | Test Acc: 99.1000% | Test Task Loss: 0.0053 | Time: 6.17s\n",
      "  Epoch 005/20 | Train Total Loss: 0.0072 | Test Acc: 98.7700% | Test Task Loss: 0.0068 | Time: 5.87s\n",
      "  Epoch 006/20 | Train Total Loss: 0.0068 | Test Acc: 99.1200% | Test Task Loss: 0.0050 | Time: 5.78s\n",
      "  Epoch 007/20 | Train Total Loss: 0.0058 | Test Acc: 99.1500% | Test Task Loss: 0.0050 | Time: 5.79s\n",
      "  Epoch 008/20 | Train Total Loss: 0.0055 | Test Acc: 98.7300% | Test Task Loss: 0.0072 | Time: 5.55s\n",
      "  Epoch 009/20 | Train Total Loss: 0.0044 | Test Acc: 98.9400% | Test Task Loss: 0.0067 | Time: 5.91s\n",
      "  Epoch 010/20 | Train Total Loss: 0.0039 | Test Acc: 98.9400% | Test Task Loss: 0.0059 | Time: 5.99s\n",
      "  Epoch 011/20 | Train Total Loss: 0.0034 | Test Acc: 99.3800% | Test Task Loss: 0.0042 | Time: 5.82s\n",
      "  Epoch 012/20 | Train Total Loss: 0.0031 | Test Acc: 98.8900% | Test Task Loss: 0.0066 | Time: 5.93s\n",
      "  Epoch 013/20 | Train Total Loss: 0.0033 | Test Acc: 98.8300% | Test Task Loss: 0.0073 | Time: 5.65s\n",
      "  Epoch 014/20 | Train Total Loss: 0.0028 | Test Acc: 99.3200% | Test Task Loss: 0.0039 | Time: 5.98s\n",
      "  Epoch 015/20 | Train Total Loss: 0.0023 | Test Acc: 99.0500% | Test Task Loss: 0.0058 | Time: 6.02s\n",
      "  Epoch 016/20 | Train Total Loss: 0.0023 | Test Acc: 99.2700% | Test Task Loss: 0.0051 | Time: 6.08s\n",
      "  Epoch 017/20 | Train Total Loss: 0.0019 | Test Acc: 99.2200% | Test Task Loss: 0.0052 | Time: 5.83s\n",
      "  Epoch 018/20 | Train Total Loss: 0.0020 | Test Acc: 99.2400% | Test Task Loss: 0.0051 | Time: 5.95s\n",
      "  Epoch 019/20 | Train Total Loss: 0.0019 | Test Acc: 99.5000% | Test Task Loss: 0.0038 | Time: 5.91s\n",
      "  Epoch 020/20 | Train Total Loss: 0.0020 | Test Acc: 99.0800% | Test Task Loss: 0.0064 | Time: 5.88s\n",
      "--- Run 2 completed in 117.55 seconds. Final Test Accuracy: 99.0800% ---\n",
      "\n",
      "--- Starting Run 3/3 for MNIST ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.0621 | Test Acc: 97.4900% | Test Task Loss: 0.0142 | Time: 5.91s\n",
      "  Epoch 002/20 | Train Total Loss: 0.0147 | Test Acc: 98.2800% | Test Task Loss: 0.0094 | Time: 6.06s\n",
      "  Epoch 003/20 | Train Total Loss: 0.0101 | Test Acc: 98.7700% | Test Task Loss: 0.0066 | Time: 6.13s\n",
      "  Epoch 004/20 | Train Total Loss: 0.0082 | Test Acc: 98.6600% | Test Task Loss: 0.0080 | Time: 6.03s\n",
      "  Epoch 005/20 | Train Total Loss: 0.0074 | Test Acc: 98.7800% | Test Task Loss: 0.0066 | Time: 6.09s\n",
      "  Epoch 006/20 | Train Total Loss: 0.0058 | Test Acc: 99.2300% | Test Task Loss: 0.0040 | Time: 5.89s\n",
      "  Epoch 007/20 | Train Total Loss: 0.0052 | Test Acc: 99.1200% | Test Task Loss: 0.0047 | Time: 5.87s\n",
      "  Epoch 008/20 | Train Total Loss: 0.0051 | Test Acc: 99.0800% | Test Task Loss: 0.0052 | Time: 6.06s\n",
      "  Epoch 009/20 | Train Total Loss: 0.0058 | Test Acc: 99.2100% | Test Task Loss: 0.0046 | Time: 5.98s\n",
      "  Epoch 010/20 | Train Total Loss: 0.0055 | Test Acc: 99.2900% | Test Task Loss: 0.0041 | Time: 6.48s\n",
      "  Epoch 011/20 | Train Total Loss: 0.0039 | Test Acc: 99.1900% | Test Task Loss: 0.0048 | Time: 6.47s\n",
      "  Epoch 012/20 | Train Total Loss: 0.0037 | Test Acc: 99.0400% | Test Task Loss: 0.0066 | Time: 6.02s\n",
      "  Epoch 013/20 | Train Total Loss: 0.0029 | Test Acc: 99.3000% | Test Task Loss: 0.0046 | Time: 6.12s\n",
      "  Epoch 014/20 | Train Total Loss: 0.0028 | Test Acc: 99.1800% | Test Task Loss: 0.0047 | Time: 6.12s\n",
      "  Epoch 015/20 | Train Total Loss: 0.0026 | Test Acc: 99.3700% | Test Task Loss: 0.0045 | Time: 5.91s\n",
      "  Epoch 016/20 | Train Total Loss: 0.0021 | Test Acc: 99.2100% | Test Task Loss: 0.0051 | Time: 6.00s\n",
      "  Epoch 017/20 | Train Total Loss: 0.0021 | Test Acc: 99.3000% | Test Task Loss: 0.0043 | Time: 6.03s\n",
      "  Epoch 018/20 | Train Total Loss: 0.0019 | Test Acc: 99.2000% | Test Task Loss: 0.0057 | Time: 5.85s\n",
      "  Epoch 019/20 | Train Total Loss: 0.0021 | Test Acc: 99.4500% | Test Task Loss: 0.0042 | Time: 6.04s\n",
      "  Epoch 020/20 | Train Total Loss: 0.0019 | Test Acc: 99.3000% | Test Task Loss: 0.0052 | Time: 6.01s\n",
      "--- Run 3 completed in 121.08 seconds. Final Test Accuracy: 99.3000% ---\n",
      "\n",
      "=== Averaged Final Epoch Metrics (PyTorch) ===\n",
      "Train Total Loss: 0.0019 ± 0.0001\n",
      "Test Task Loss:   0.0053 ± 0.0009\n",
      "Test Accuracy:    99.24% ± 0.12%\n",
      "Non-Negativity Loss:0.0000 ± 0.0000\n",
      "Convexity Loss:   0.0000 ± 0.0000\n",
      "Lipschitz Loss:   0.0049 ± 0.0010\n",
      "Average Run Time: 117.73 seconds\n",
      "Detailed plot saved to: plots_learnable_loss_detailed_pytorch/results_mnist_learnable_loss_detailed_plots_pytorch.png\n",
      "\n",
      "=== Generating Final Classification Report & F1 Plot (Last Run) ===\n",
      "\n",
      "Classification Metrics (sklearn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9949    0.9980    0.9964       980\n",
      "           1     0.9982    0.9938    0.9960      1135\n",
      "           2     0.9875    0.9971    0.9923      1032\n",
      "           3     0.9882    0.9990    0.9936      1010\n",
      "           4     0.9990    0.9817    0.9902       982\n",
      "           5     0.9955    0.9899    0.9927       892\n",
      "           6     0.9969    0.9937    0.9953       958\n",
      "           7     0.9971    0.9893    0.9932      1028\n",
      "           8     0.9938    0.9928    0.9933       974\n",
      "           9     0.9795    0.9941    0.9867      1009\n",
      "\n",
      "    accuracy                         0.9930     10000\n",
      "   macro avg     0.9931    0.9929    0.9930     10000\n",
      "weighted avg     0.9930    0.9930    0.9930     10000\n",
      "\n",
      "F1 score plot saved to: plots_learnable_loss_detailed_pytorch/f1_scores_mnist_pytorch.png\n",
      "--- Completed processing MNIST in 356.48 seconds ---\n",
      "\n",
      "==================== Processing Dataset: FASHION_MNIST ====================\n",
      "Loading and preprocessing data...\n",
      "Data loading complete. Train samples: 60000, Test samples: 10000\n",
      "\n",
      "--- Starting Run 1/3 for FASHION_MNIST ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.1046 | Test Acc: 87.6300% | Test Task Loss: 0.0595 | Time: 5.47s\n",
      "  Epoch 002/20 | Train Total Loss: 0.0594 | Test Acc: 89.2800% | Test Task Loss: 0.0522 | Time: 5.57s\n",
      "  Epoch 003/20 | Train Total Loss: 0.0517 | Test Acc: 90.6500% | Test Task Loss: 0.0464 | Time: 5.59s\n",
      "  Epoch 004/20 | Train Total Loss: 0.0439 | Test Acc: 90.7700% | Test Task Loss: 0.0455 | Time: 5.52s\n",
      "  Epoch 005/20 | Train Total Loss: 0.0405 | Test Acc: 91.0900% | Test Task Loss: 0.0451 | Time: 5.50s\n",
      "  Epoch 006/20 | Train Total Loss: 0.0363 | Test Acc: 91.5400% | Test Task Loss: 0.0410 | Time: 5.54s\n",
      "  Epoch 007/20 | Train Total Loss: 0.0341 | Test Acc: 90.8600% | Test Task Loss: 0.0462 | Time: 5.57s\n",
      "  Epoch 008/20 | Train Total Loss: 0.0319 | Test Acc: 91.6800% | Test Task Loss: 0.0419 | Time: 5.57s\n",
      "  Epoch 009/20 | Train Total Loss: 0.0296 | Test Acc: 91.4800% | Test Task Loss: 0.0435 | Time: 5.53s\n",
      "  Epoch 010/20 | Train Total Loss: 0.0267 | Test Acc: 92.1600% | Test Task Loss: 0.0408 | Time: 5.47s\n",
      "  Epoch 011/20 | Train Total Loss: 0.0254 | Test Acc: 91.3100% | Test Task Loss: 0.0475 | Time: 5.60s\n",
      "  Epoch 012/20 | Train Total Loss: 0.0232 | Test Acc: 92.0600% | Test Task Loss: 0.0445 | Time: 5.40s\n",
      "  Epoch 013/20 | Train Total Loss: 0.0205 | Test Acc: 91.4400% | Test Task Loss: 0.0473 | Time: 5.38s\n",
      "  Epoch 014/20 | Train Total Loss: 0.0197 | Test Acc: 91.2800% | Test Task Loss: 0.0515 | Time: 5.25s\n",
      "  Epoch 015/20 | Train Total Loss: 0.0186 | Test Acc: 92.2600% | Test Task Loss: 0.0484 | Time: 5.18s\n",
      "  Epoch 016/20 | Train Total Loss: 0.0183 | Test Acc: 92.6000% | Test Task Loss: 0.0459 | Time: 5.23s\n",
      "  Epoch 017/20 | Train Total Loss: 0.0174 | Test Acc: 92.4300% | Test Task Loss: 0.0485 | Time: 5.67s\n",
      "  Epoch 018/20 | Train Total Loss: 0.0165 | Test Acc: 92.5200% | Test Task Loss: 0.0512 | Time: 5.55s\n",
      "  Epoch 019/20 | Train Total Loss: 0.0151 | Test Acc: 92.2800% | Test Task Loss: 0.0518 | Time: 5.65s\n",
      "  Epoch 020/20 | Train Total Loss: 0.0130 | Test Acc: 91.9000% | Test Task Loss: 0.0532 | Time: 5.36s\n",
      "--- Run 1 completed in 109.59 seconds. Final Test Accuracy: 91.9000% ---\n",
      "\n",
      "--- Starting Run 2/3 for FASHION_MNIST ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.1115 | Test Acc: 87.2500% | Test Task Loss: 0.0617 | Time: 5.53s\n",
      "  Epoch 002/20 | Train Total Loss: 0.0603 | Test Acc: 89.8400% | Test Task Loss: 0.0500 | Time: 5.51s\n",
      "  Epoch 003/20 | Train Total Loss: 0.0509 | Test Acc: 90.3300% | Test Task Loss: 0.0466 | Time: 5.44s\n",
      "  Epoch 004/20 | Train Total Loss: 0.0454 | Test Acc: 89.0500% | Test Task Loss: 0.0531 | Time: 5.47s\n",
      "  Epoch 005/20 | Train Total Loss: 0.0421 | Test Acc: 89.8800% | Test Task Loss: 0.0527 | Time: 5.24s\n",
      "  Epoch 006/20 | Train Total Loss: 0.0377 | Test Acc: 91.5500% | Test Task Loss: 0.0419 | Time: 5.38s\n",
      "  Epoch 007/20 | Train Total Loss: 0.0348 | Test Acc: 91.8500% | Test Task Loss: 0.0411 | Time: 5.55s\n",
      "  Epoch 008/20 | Train Total Loss: 0.0319 | Test Acc: 90.8800% | Test Task Loss: 0.0459 | Time: 5.67s\n",
      "  Epoch 009/20 | Train Total Loss: 0.0296 | Test Acc: 91.7900% | Test Task Loss: 0.0435 | Time: 5.51s\n",
      "  Epoch 010/20 | Train Total Loss: 0.0286 | Test Acc: 91.5400% | Test Task Loss: 0.0450 | Time: 5.29s\n",
      "  Epoch 011/20 | Train Total Loss: 0.0257 | Test Acc: 91.5600% | Test Task Loss: 0.0456 | Time: 5.54s\n",
      "  Epoch 012/20 | Train Total Loss: 0.0257 | Test Acc: 92.0000% | Test Task Loss: 0.0439 | Time: 5.64s\n",
      "  Epoch 013/20 | Train Total Loss: 0.0220 | Test Acc: 92.0400% | Test Task Loss: 0.0457 | Time: 5.41s\n",
      "  Epoch 014/20 | Train Total Loss: 0.0205 | Test Acc: 92.4300% | Test Task Loss: 0.0435 | Time: 5.31s\n",
      "  Epoch 015/20 | Train Total Loss: 0.0194 | Test Acc: 92.1500% | Test Task Loss: 0.0437 | Time: 5.27s\n",
      "  Epoch 016/20 | Train Total Loss: 0.0173 | Test Acc: 92.2700% | Test Task Loss: 0.0461 | Time: 5.23s\n",
      "  Epoch 017/20 | Train Total Loss: 0.0171 | Test Acc: 92.0200% | Test Task Loss: 0.0497 | Time: 5.33s\n",
      "  Epoch 018/20 | Train Total Loss: 0.0159 | Test Acc: 91.7500% | Test Task Loss: 0.0541 | Time: 5.68s\n",
      "  Epoch 019/20 | Train Total Loss: 0.0144 | Test Acc: 91.8100% | Test Task Loss: 0.0486 | Time: 5.32s\n",
      "  Epoch 020/20 | Train Total Loss: 0.0134 | Test Acc: 92.4600% | Test Task Loss: 0.0498 | Time: 5.39s\n",
      "--- Run 2 completed in 108.74 seconds. Final Test Accuracy: 92.4600% ---\n",
      "\n",
      "--- Starting Run 3/3 for FASHION_MNIST ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.1105 | Test Acc: 86.9800% | Test Task Loss: 0.0642 | Time: 5.47s\n",
      "  Epoch 002/20 | Train Total Loss: 0.0603 | Test Acc: 88.4800% | Test Task Loss: 0.0552 | Time: 5.42s\n",
      "  Epoch 003/20 | Train Total Loss: 0.0512 | Test Acc: 90.4400% | Test Task Loss: 0.0466 | Time: 5.68s\n",
      "  Epoch 004/20 | Train Total Loss: 0.0463 | Test Acc: 90.1400% | Test Task Loss: 0.0482 | Time: 5.64s\n",
      "  Epoch 005/20 | Train Total Loss: 0.0425 | Test Acc: 91.1000% | Test Task Loss: 0.0431 | Time: 5.38s\n",
      "  Epoch 006/20 | Train Total Loss: 0.0378 | Test Acc: 90.2400% | Test Task Loss: 0.0495 | Time: 5.49s\n",
      "  Epoch 007/20 | Train Total Loss: 0.0345 | Test Acc: 91.6800% | Test Task Loss: 0.0410 | Time: 5.61s\n",
      "  Epoch 008/20 | Train Total Loss: 0.0316 | Test Acc: 91.0000% | Test Task Loss: 0.0446 | Time: 5.47s\n",
      "  Epoch 009/20 | Train Total Loss: 0.0300 | Test Acc: 91.6500% | Test Task Loss: 0.0415 | Time: 5.48s\n",
      "  Epoch 010/20 | Train Total Loss: 0.0277 | Test Acc: 91.4300% | Test Task Loss: 0.0447 | Time: 5.31s\n",
      "  Epoch 011/20 | Train Total Loss: 0.0256 | Test Acc: 90.7500% | Test Task Loss: 0.0482 | Time: 5.48s\n",
      "  Epoch 012/20 | Train Total Loss: 0.0237 | Test Acc: 92.3600% | Test Task Loss: 0.0413 | Time: 5.73s\n",
      "  Epoch 013/20 | Train Total Loss: 0.0213 | Test Acc: 90.9800% | Test Task Loss: 0.0462 | Time: 5.81s\n",
      "  Epoch 014/20 | Train Total Loss: 0.0205 | Test Acc: 91.9400% | Test Task Loss: 0.0479 | Time: 5.45s\n",
      "  Epoch 015/20 | Train Total Loss: 0.0188 | Test Acc: 91.7600% | Test Task Loss: 0.0509 | Time: 5.56s\n",
      "  Epoch 016/20 | Train Total Loss: 0.0182 | Test Acc: 91.9600% | Test Task Loss: 0.0509 | Time: 5.49s\n",
      "  Epoch 017/20 | Train Total Loss: 0.0174 | Test Acc: 92.0500% | Test Task Loss: 0.0473 | Time: 5.46s\n",
      "  Epoch 018/20 | Train Total Loss: 0.0161 | Test Acc: 92.2200% | Test Task Loss: 0.0483 | Time: 5.46s\n",
      "  Epoch 019/20 | Train Total Loss: 0.0147 | Test Acc: 91.8500% | Test Task Loss: 0.0551 | Time: 5.64s\n",
      "  Epoch 020/20 | Train Total Loss: 0.0130 | Test Acc: 92.2700% | Test Task Loss: 0.0549 | Time: 5.20s\n",
      "--- Run 3 completed in 110.26 seconds. Final Test Accuracy: 92.2700% ---\n",
      "\n",
      "=== Averaged Final Epoch Metrics (PyTorch) ===\n",
      "Train Total Loss: 0.0131 ± 0.0002\n",
      "Test Task Loss:   0.0526 ± 0.0022\n",
      "Test Accuracy:    92.21% ± 0.23%\n",
      "Non-Negativity Loss:0.0000 ± 0.0000\n",
      "Convexity Loss:   0.0000 ± 0.0000\n",
      "Lipschitz Loss:   0.0049 ± 0.0008\n",
      "Average Run Time: 109.53 seconds\n",
      "Detailed plot saved to: plots_learnable_loss_detailed_pytorch/results_fashion_mnist_learnable_loss_detailed_plots_pytorch.png\n",
      "\n",
      "=== Generating Final Classification Report & F1 Plot (Last Run) ===\n",
      "\n",
      "Classification Metrics (sklearn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8684    0.8910    0.8796      1000\n",
      "           1     0.9949    0.9800    0.9874      1000\n",
      "           2     0.9051    0.8490    0.8762      1000\n",
      "           3     0.9239    0.9230    0.9235      1000\n",
      "           4     0.8200    0.9340    0.8733      1000\n",
      "           5     0.9899    0.9790    0.9844      1000\n",
      "           6     0.8233    0.7410    0.7800      1000\n",
      "           7     0.9633    0.9710    0.9671      1000\n",
      "           8     0.9742    0.9830    0.9786      1000\n",
      "           9     0.9692    0.9760    0.9726      1000\n",
      "\n",
      "    accuracy                         0.9227     10000\n",
      "   macro avg     0.9232    0.9227    0.9223     10000\n",
      "weighted avg     0.9232    0.9227    0.9223     10000\n",
      "\n",
      "F1 score plot saved to: plots_learnable_loss_detailed_pytorch/f1_scores_fashion_mnist_pytorch.png\n",
      "--- Completed processing FASHION_MNIST in 331.50 seconds ---\n",
      "\n",
      "==================== Processing Dataset: CIFAR10 ====================\n",
      "Loading and preprocessing data...\n",
      "Data loading complete. Train samples: 50000, Test samples: 10000\n",
      "\n",
      "--- Starting Run 1/3 for CIFAR10 ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.2691 | Test Acc: 35.7000% | Test Task Loss: 0.2640 | Time: 5.29s\n",
      "  Epoch 002/20 | Train Total Loss: 0.2078 | Test Acc: 60.1600% | Test Task Loss: 0.1775 | Time: 5.23s\n",
      "  Epoch 003/20 | Train Total Loss: 0.1826 | Test Acc: 63.4700% | Test Task Loss: 0.1647 | Time: 5.15s\n",
      "  Epoch 004/20 | Train Total Loss: 0.1680 | Test Acc: 68.8100% | Test Task Loss: 0.1472 | Time: 5.12s\n",
      "  Epoch 005/20 | Train Total Loss: 0.1573 | Test Acc: 67.4800% | Test Task Loss: 0.1470 | Time: 5.14s\n",
      "  Epoch 006/20 | Train Total Loss: 0.1486 | Test Acc: 69.8800% | Test Task Loss: 0.1410 | Time: 5.10s\n",
      "  Epoch 007/20 | Train Total Loss: 0.1416 | Test Acc: 69.3300% | Test Task Loss: 0.1421 | Time: 5.13s\n",
      "  Epoch 008/20 | Train Total Loss: 0.1343 | Test Acc: 71.5900% | Test Task Loss: 0.1329 | Time: 5.09s\n",
      "  Epoch 009/20 | Train Total Loss: 0.1287 | Test Acc: 70.1800% | Test Task Loss: 0.1427 | Time: 5.25s\n",
      "  Epoch 010/20 | Train Total Loss: 0.1239 | Test Acc: 71.0900% | Test Task Loss: 0.1372 | Time: 5.15s\n",
      "  Epoch 011/20 | Train Total Loss: 0.1192 | Test Acc: 70.4200% | Test Task Loss: 0.1382 | Time: 5.33s\n",
      "  Epoch 012/20 | Train Total Loss: 0.1137 | Test Acc: 72.5600% | Test Task Loss: 0.1321 | Time: 5.08s\n",
      "  Epoch 013/20 | Train Total Loss: 0.1102 | Test Acc: 72.7200% | Test Task Loss: 0.1332 | Time: 5.29s\n",
      "  Epoch 014/20 | Train Total Loss: 0.1058 | Test Acc: 72.9500% | Test Task Loss: 0.1307 | Time: 5.23s\n",
      "  Epoch 015/20 | Train Total Loss: 0.1022 | Test Acc: 74.7000% | Test Task Loss: 0.1214 | Time: 5.19s\n",
      "  Epoch 016/20 | Train Total Loss: 0.0991 | Test Acc: 74.9100% | Test Task Loss: 0.1245 | Time: 5.09s\n",
      "  Epoch 017/20 | Train Total Loss: 0.0970 | Test Acc: 75.4700% | Test Task Loss: 0.1213 | Time: 5.18s\n",
      "  Epoch 018/20 | Train Total Loss: 0.0918 | Test Acc: 70.8400% | Test Task Loss: 0.1527 | Time: 5.25s\n",
      "  Epoch 019/20 | Train Total Loss: 0.0881 | Test Acc: 76.5200% | Test Task Loss: 0.1206 | Time: 5.51s\n",
      "  Epoch 020/20 | Train Total Loss: 0.0858 | Test Acc: 74.3100% | Test Task Loss: 0.1373 | Time: 5.60s\n",
      "--- Run 1 completed in 104.40 seconds. Final Test Accuracy: 74.3100% ---\n",
      "\n",
      "--- Starting Run 2/3 for CIFAR10 ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.2651 | Test Acc: 42.3400% | Test Task Loss: 0.2433 | Time: 5.20s\n",
      "  Epoch 002/20 | Train Total Loss: 0.2055 | Test Acc: 62.6500% | Test Task Loss: 0.1716 | Time: 5.29s\n",
      "  Epoch 003/20 | Train Total Loss: 0.1817 | Test Acc: 61.7300% | Test Task Loss: 0.1767 | Time: 5.01s\n",
      "  Epoch 004/20 | Train Total Loss: 0.1668 | Test Acc: 65.1700% | Test Task Loss: 0.1583 | Time: 5.27s\n",
      "  Epoch 005/20 | Train Total Loss: 0.1542 | Test Acc: 70.3100% | Test Task Loss: 0.1401 | Time: 5.29s\n",
      "  Epoch 006/20 | Train Total Loss: 0.1466 | Test Acc: 68.4700% | Test Task Loss: 0.1484 | Time: 5.04s\n",
      "  Epoch 007/20 | Train Total Loss: 0.1389 | Test Acc: 70.5600% | Test Task Loss: 0.1400 | Time: 5.06s\n",
      "  Epoch 008/20 | Train Total Loss: 0.1320 | Test Acc: 70.6100% | Test Task Loss: 0.1392 | Time: 5.14s\n",
      "  Epoch 009/20 | Train Total Loss: 0.1272 | Test Acc: 71.6300% | Test Task Loss: 0.1328 | Time: 5.42s\n",
      "  Epoch 010/20 | Train Total Loss: 0.1222 | Test Acc: 72.6600% | Test Task Loss: 0.1291 | Time: 5.33s\n",
      "  Epoch 011/20 | Train Total Loss: 0.1176 | Test Acc: 75.0500% | Test Task Loss: 0.1195 | Time: 5.54s\n",
      "  Epoch 012/20 | Train Total Loss: 0.1114 | Test Acc: 75.7800% | Test Task Loss: 0.1160 | Time: 5.40s\n",
      "  Epoch 013/20 | Train Total Loss: 0.1089 | Test Acc: 75.9100% | Test Task Loss: 0.1160 | Time: 5.33s\n",
      "  Epoch 014/20 | Train Total Loss: 0.1033 | Test Acc: 73.8000% | Test Task Loss: 0.1277 | Time: 5.49s\n",
      "  Epoch 015/20 | Train Total Loss: 0.0990 | Test Acc: 73.9700% | Test Task Loss: 0.1275 | Time: 5.39s\n",
      "  Epoch 016/20 | Train Total Loss: 0.0964 | Test Acc: 71.4000% | Test Task Loss: 0.1391 | Time: 5.56s\n",
      "  Epoch 017/20 | Train Total Loss: 0.0909 | Test Acc: 75.7900% | Test Task Loss: 0.1226 | Time: 5.43s\n",
      "  Epoch 018/20 | Train Total Loss: 0.0883 | Test Acc: 74.3900% | Test Task Loss: 0.1307 | Time: 5.46s\n",
      "  Epoch 019/20 | Train Total Loss: 0.0842 | Test Acc: 75.4900% | Test Task Loss: 0.1229 | Time: 5.48s\n",
      "  Epoch 020/20 | Train Total Loss: 0.0841 | Test Acc: 75.6800% | Test Task Loss: 0.1251 | Time: 5.40s\n",
      "--- Run 2 completed in 106.55 seconds. Final Test Accuracy: 75.6800% ---\n",
      "\n",
      "--- Starting Run 3/3 for CIFAR10 ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.2991 | Test Acc: 36.0300% | Test Task Loss: 0.2566 | Time: 5.50s\n",
      "  Epoch 002/20 | Train Total Loss: 0.2371 | Test Acc: 56.7400% | Test Task Loss: 0.1962 | Time: 5.49s\n",
      "  Epoch 003/20 | Train Total Loss: 0.2136 | Test Acc: 59.8500% | Test Task Loss: 0.1843 | Time: 5.53s\n",
      "  Epoch 004/20 | Train Total Loss: 0.1970 | Test Acc: 65.2400% | Test Task Loss: 0.1595 | Time: 5.42s\n",
      "  Epoch 005/20 | Train Total Loss: 0.1856 | Test Acc: 62.6100% | Test Task Loss: 0.1746 | Time: 5.64s\n",
      "  Epoch 006/20 | Train Total Loss: 0.1785 | Test Acc: 68.5600% | Test Task Loss: 0.1472 | Time: 5.49s\n",
      "  Epoch 007/20 | Train Total Loss: 0.1724 | Test Acc: 68.4700% | Test Task Loss: 0.1488 | Time: 5.39s\n",
      "  Epoch 008/20 | Train Total Loss: 0.1646 | Test Acc: 68.2800% | Test Task Loss: 0.1479 | Time: 5.31s\n",
      "  Epoch 009/20 | Train Total Loss: 0.1605 | Test Acc: 72.3500% | Test Task Loss: 0.1340 | Time: 5.59s\n",
      "  Epoch 010/20 | Train Total Loss: 0.1558 | Test Acc: 70.1600% | Test Task Loss: 0.1407 | Time: 5.76s\n",
      "  Epoch 011/20 | Train Total Loss: 0.1516 | Test Acc: 66.8800% | Test Task Loss: 0.1540 | Time: 5.48s\n",
      "  Epoch 012/20 | Train Total Loss: 0.1481 | Test Acc: 73.0300% | Test Task Loss: 0.1277 | Time: 5.45s\n",
      "  Epoch 013/20 | Train Total Loss: 0.1440 | Test Acc: 71.6500% | Test Task Loss: 0.1357 | Time: 5.12s\n",
      "  Epoch 014/20 | Train Total Loss: 0.1405 | Test Acc: 67.1100% | Test Task Loss: 0.1531 | Time: 5.41s\n",
      "  Epoch 015/20 | Train Total Loss: 0.1386 | Test Acc: 69.3400% | Test Task Loss: 0.1525 | Time: 5.17s\n",
      "  Epoch 016/20 | Train Total Loss: 0.1347 | Test Acc: 71.8800% | Test Task Loss: 0.1337 | Time: 5.07s\n",
      "  Epoch 017/20 | Train Total Loss: 0.1309 | Test Acc: 73.3500% | Test Task Loss: 0.1288 | Time: 5.03s\n",
      "  Epoch 018/20 | Train Total Loss: 0.1291 | Test Acc: 72.1200% | Test Task Loss: 0.1373 | Time: 5.16s\n",
      "  Epoch 019/20 | Train Total Loss: 0.1246 | Test Acc: 71.6700% | Test Task Loss: 0.1359 | Time: 5.02s\n",
      "  Epoch 020/20 | Train Total Loss: 0.1227 | Test Acc: 73.6300% | Test Task Loss: 0.1248 | Time: 5.34s\n",
      "--- Run 3 completed in 107.47 seconds. Final Test Accuracy: 73.6300% ---\n",
      "\n",
      "=== Averaged Final Epoch Metrics (PyTorch) ===\n",
      "Train Total Loss: 0.0975 ± 0.0178\n",
      "Test Task Loss:   0.1291 ± 0.0058\n",
      "Test Accuracy:    74.54% ± 0.85%\n",
      "Non-Negativity Loss:0.0000 ± 0.0000\n",
      "Convexity Loss:   0.0000 ± 0.0000\n",
      "Lipschitz Loss:   0.0056 ± 0.0008\n",
      "Average Run Time: 106.14 seconds\n",
      "Detailed plot saved to: plots_learnable_loss_detailed_pytorch/results_cifar10_learnable_loss_detailed_plots_pytorch.png\n",
      "\n",
      "=== Generating Final Classification Report & F1 Plot (Last Run) ===\n",
      "\n",
      "Classification Metrics (sklearn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.8700    0.7549      1000\n",
      "           1     0.8567    0.8610    0.8589      1000\n",
      "           2     0.6578    0.6460    0.6519      1000\n",
      "           3     0.5175    0.6360    0.5707      1000\n",
      "           4     0.8336    0.5310    0.6487      1000\n",
      "           5     0.8030    0.4890    0.6078      1000\n",
      "           6     0.7685    0.8400    0.8027      1000\n",
      "           7     0.7250    0.8200    0.7696      1000\n",
      "           8     0.8376    0.8510    0.8442      1000\n",
      "           9     0.8248    0.8190    0.8219      1000\n",
      "\n",
      "    accuracy                         0.7363     10000\n",
      "   macro avg     0.7491    0.7363    0.7331     10000\n",
      "weighted avg     0.7491    0.7363    0.7331     10000\n",
      "\n",
      "F1 score plot saved to: plots_learnable_loss_detailed_pytorch/f1_scores_cifar10_pytorch.png\n",
      "--- Completed processing CIFAR10 in 323.31 seconds ---\n",
      "\n",
      "==================== Processing Dataset: CIFAR100 ====================\n",
      "Loading and preprocessing data...\n",
      "Data loading complete. Train samples: 50000, Test samples: 10000\n",
      "\n",
      "--- Starting Run 1/3 for CIFAR100 ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.0555 | Test Acc: 11.1700% | Test Task Loss: 0.0497 | Time: 33.73s\n",
      "  Epoch 002/20 | Train Total Loss: 0.0492 | Test Acc: 19.5200% | Test Task Loss: 0.0441 | Time: 33.35s\n",
      "  Epoch 003/20 | Train Total Loss: 0.0454 | Test Acc: 24.1200% | Test Task Loss: 0.0413 | Time: 33.86s\n",
      "  Epoch 004/20 | Train Total Loss: 0.0429 | Test Acc: 27.4900% | Test Task Loss: 0.0390 | Time: 33.93s\n",
      "  Epoch 005/20 | Train Total Loss: 0.0410 | Test Acc: 31.9800% | Test Task Loss: 0.0365 | Time: 33.07s\n",
      "  Epoch 006/20 | Train Total Loss: 0.0396 | Test Acc: 33.6400% | Test Task Loss: 0.0355 | Time: 34.07s\n",
      "  Epoch 007/20 | Train Total Loss: 0.0385 | Test Acc: 33.3900% | Test Task Loss: 0.0349 | Time: 33.59s\n",
      "  Epoch 008/20 | Train Total Loss: 0.0378 | Test Acc: 35.6600% | Test Task Loss: 0.0340 | Time: 34.06s\n",
      "  Epoch 009/20 | Train Total Loss: 0.0369 | Test Acc: 37.0200% | Test Task Loss: 0.0336 | Time: 33.93s\n",
      "  Epoch 010/20 | Train Total Loss: 0.0361 | Test Acc: 36.6800% | Test Task Loss: 0.0336 | Time: 33.75s\n",
      "  Epoch 011/20 | Train Total Loss: 0.0356 | Test Acc: 37.7800% | Test Task Loss: 0.0326 | Time: 34.24s\n",
      "  Epoch 012/20 | Train Total Loss: 0.0349 | Test Acc: 37.8600% | Test Task Loss: 0.0326 | Time: 34.01s\n",
      "  Epoch 013/20 | Train Total Loss: 0.0344 | Test Acc: 34.8000% | Test Task Loss: 0.0349 | Time: 33.80s\n",
      "  Epoch 014/20 | Train Total Loss: 0.0340 | Test Acc: 39.1400% | Test Task Loss: 0.0320 | Time: 34.39s\n",
      "  Epoch 015/20 | Train Total Loss: 0.0334 | Test Acc: 38.6900% | Test Task Loss: 0.0322 | Time: 33.96s\n",
      "  Epoch 016/20 | Train Total Loss: 0.0328 | Test Acc: 39.1500% | Test Task Loss: 0.0321 | Time: 33.99s\n",
      "  Epoch 017/20 | Train Total Loss: 0.0325 | Test Acc: 39.4700% | Test Task Loss: 0.0318 | Time: 33.88s\n",
      "  Epoch 018/20 | Train Total Loss: 0.0320 | Test Acc: 37.7800% | Test Task Loss: 0.0328 | Time: 33.89s\n",
      "  Epoch 019/20 | Train Total Loss: 0.0315 | Test Acc: 40.8200% | Test Task Loss: 0.0312 | Time: 33.72s\n",
      "  Epoch 020/20 | Train Total Loss: 0.0311 | Test Acc: 37.6700% | Test Task Loss: 0.0329 | Time: 33.52s\n",
      "--- Run 1 completed in 676.77 seconds. Final Test Accuracy: 37.6700% ---\n",
      "\n",
      "--- Starting Run 2/3 for CIFAR100 ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.0572 | Test Acc: 10.0700% | Test Task Loss: 0.0500 | Time: 33.80s\n",
      "  Epoch 002/20 | Train Total Loss: 0.0492 | Test Acc: 20.7000% | Test Task Loss: 0.0437 | Time: 33.79s\n",
      "  Epoch 003/20 | Train Total Loss: 0.0458 | Test Acc: 23.0800% | Test Task Loss: 0.0416 | Time: 34.56s\n",
      "  Epoch 004/20 | Train Total Loss: 0.0435 | Test Acc: 27.5300% | Test Task Loss: 0.0389 | Time: 34.27s\n",
      "  Epoch 005/20 | Train Total Loss: 0.0418 | Test Acc: 28.2400% | Test Task Loss: 0.0381 | Time: 34.81s\n",
      "  Epoch 006/20 | Train Total Loss: 0.0407 | Test Acc: 31.9200% | Test Task Loss: 0.0361 | Time: 34.92s\n",
      "  Epoch 007/20 | Train Total Loss: 0.0396 | Test Acc: 34.5100% | Test Task Loss: 0.0350 | Time: 35.03s\n",
      "  Epoch 008/20 | Train Total Loss: 0.0388 | Test Acc: 33.4800% | Test Task Loss: 0.0352 | Time: 35.14s\n",
      "  Epoch 009/20 | Train Total Loss: 0.0382 | Test Acc: 35.4800% | Test Task Loss: 0.0341 | Time: 35.11s\n",
      "  Epoch 010/20 | Train Total Loss: 0.0376 | Test Acc: 33.7700% | Test Task Loss: 0.0347 | Time: 34.74s\n",
      "  Epoch 011/20 | Train Total Loss: 0.0371 | Test Acc: 35.5800% | Test Task Loss: 0.0340 | Time: 35.19s\n",
      "  Epoch 012/20 | Train Total Loss: 0.0364 | Test Acc: 35.0300% | Test Task Loss: 0.0349 | Time: 36.16s\n",
      "  Epoch 013/20 | Train Total Loss: 0.0361 | Test Acc: 36.3300% | Test Task Loss: 0.0336 | Time: 34.81s\n",
      "  Epoch 014/20 | Train Total Loss: 0.0354 | Test Acc: 36.6300% | Test Task Loss: 0.0334 | Time: 34.11s\n",
      "  Epoch 015/20 | Train Total Loss: 0.0350 | Test Acc: 38.8300% | Test Task Loss: 0.0326 | Time: 33.68s\n",
      "  Epoch 016/20 | Train Total Loss: 0.0345 | Test Acc: 35.8700% | Test Task Loss: 0.0335 | Time: 33.87s\n",
      "  Epoch 017/20 | Train Total Loss: 0.0341 | Test Acc: 36.1300% | Test Task Loss: 0.0336 | Time: 34.19s\n",
      "  Epoch 018/20 | Train Total Loss: 0.0337 | Test Acc: 38.6700% | Test Task Loss: 0.0319 | Time: 33.91s\n",
      "  Epoch 019/20 | Train Total Loss: 0.0334 | Test Acc: 39.8600% | Test Task Loss: 0.0317 | Time: 33.96s\n",
      "  Epoch 020/20 | Train Total Loss: 0.0330 | Test Acc: 39.6100% | Test Task Loss: 0.0318 | Time: 34.07s\n",
      "--- Run 2 completed in 690.22 seconds. Final Test Accuracy: 39.6100% ---\n",
      "\n",
      "--- Starting Run 3/3 for CIFAR100 ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.0570 | Test Acc: 9.8600% | Test Task Loss: 0.0503 | Time: 34.24s\n",
      "  Epoch 002/20 | Train Total Loss: 0.0495 | Test Acc: 18.0300% | Test Task Loss: 0.0450 | Time: 34.48s\n",
      "  Epoch 003/20 | Train Total Loss: 0.0462 | Test Acc: 24.6200% | Test Task Loss: 0.0416 | Time: 33.79s\n",
      "  Epoch 004/20 | Train Total Loss: 0.0439 | Test Acc: 26.8800% | Test Task Loss: 0.0394 | Time: 33.59s\n",
      "  Epoch 005/20 | Train Total Loss: 0.0422 | Test Acc: 29.9400% | Test Task Loss: 0.0380 | Time: 34.02s\n",
      "  Epoch 006/20 | Train Total Loss: 0.0410 | Test Acc: 31.2200% | Test Task Loss: 0.0373 | Time: 34.02s\n",
      "  Epoch 007/20 | Train Total Loss: 0.0400 | Test Acc: 31.5100% | Test Task Loss: 0.0364 | Time: 34.22s\n",
      "  Epoch 008/20 | Train Total Loss: 0.0392 | Test Acc: 32.8600% | Test Task Loss: 0.0355 | Time: 33.86s\n",
      "  Epoch 009/20 | Train Total Loss: 0.0385 | Test Acc: 35.0400% | Test Task Loss: 0.0344 | Time: 33.79s\n",
      "  Epoch 010/20 | Train Total Loss: 0.0377 | Test Acc: 32.6900% | Test Task Loss: 0.0352 | Time: 33.83s\n",
      "  Epoch 011/20 | Train Total Loss: 0.0370 | Test Acc: 35.9500% | Test Task Loss: 0.0338 | Time: 33.82s\n",
      "  Epoch 012/20 | Train Total Loss: 0.0366 | Test Acc: 36.1100% | Test Task Loss: 0.0336 | Time: 33.71s\n",
      "  Epoch 013/20 | Train Total Loss: 0.0361 | Test Acc: 37.3600% | Test Task Loss: 0.0332 | Time: 34.07s\n",
      "  Epoch 014/20 | Train Total Loss: 0.0355 | Test Acc: 38.0300% | Test Task Loss: 0.0327 | Time: 33.80s\n",
      "  Epoch 015/20 | Train Total Loss: 0.0351 | Test Acc: 36.3600% | Test Task Loss: 0.0338 | Time: 34.28s\n",
      "  Epoch 016/20 | Train Total Loss: 0.0347 | Test Acc: 37.9600% | Test Task Loss: 0.0327 | Time: 34.41s\n",
      "  Epoch 017/20 | Train Total Loss: 0.0343 | Test Acc: 39.3300% | Test Task Loss: 0.0320 | Time: 33.68s\n",
      "  Epoch 018/20 | Train Total Loss: 0.0338 | Test Acc: 35.3300% | Test Task Loss: 0.0335 | Time: 34.10s\n",
      "  Epoch 019/20 | Train Total Loss: 0.0333 | Test Acc: 38.3000% | Test Task Loss: 0.0326 | Time: 34.56s\n",
      "  Epoch 020/20 | Train Total Loss: 0.0330 | Test Acc: 38.7700% | Test Task Loss: 0.0322 | Time: 33.59s\n",
      "--- Run 3 completed in 679.97 seconds. Final Test Accuracy: 38.7700% ---\n",
      "\n",
      "=== Averaged Final Epoch Metrics (PyTorch) ===\n",
      "Train Total Loss: 0.0324 ± 0.0009\n",
      "Test Task Loss:   0.0323 ± 0.0004\n",
      "Test Accuracy:    38.68% ± 0.79%\n",
      "Non-Negativity Loss:0.0000 ± 0.0000\n",
      "Convexity Loss:   0.0000 ± 0.0000\n",
      "Lipschitz Loss:   0.0025 ± 0.0002\n",
      "Average Run Time: 682.32 seconds\n",
      "Detailed plot saved to: plots_learnable_loss_detailed_pytorch/results_cifar100_learnable_loss_detailed_plots_pytorch.png\n",
      "\n",
      "=== Generating Final Classification Report & F1 Plot (Last Run) ===\n",
      "\n",
      "Classification Metrics (sklearn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7903    0.4900    0.6049       100\n",
      "           1     0.4417    0.5300    0.4818       100\n",
      "           2     0.2604    0.2500    0.2551       100\n",
      "           3     0.1593    0.2900    0.2057       100\n",
      "           4     0.2105    0.2400    0.2243       100\n",
      "           5     0.4211    0.3200    0.3636       100\n",
      "           6     0.2819    0.5300    0.3681       100\n",
      "           7     0.3659    0.4500    0.4036       100\n",
      "           8     0.7925    0.4200    0.5490       100\n",
      "           9     0.7162    0.5300    0.6092       100\n",
      "          10     0.3816    0.2900    0.3295       100\n",
      "          11     0.3038    0.2400    0.2682       100\n",
      "          12     0.5283    0.2800    0.3660       100\n",
      "          13     0.3200    0.2400    0.2743       100\n",
      "          14     0.3091    0.1700    0.2194       100\n",
      "          15     0.3273    0.1800    0.2323       100\n",
      "          16     0.5000    0.3900    0.4382       100\n",
      "          17     0.6829    0.5600    0.6154       100\n",
      "          18     0.3025    0.3600    0.3288       100\n",
      "          19     0.5283    0.2800    0.3660       100\n",
      "          20     0.9077    0.5900    0.7152       100\n",
      "          21     0.4653    0.6700    0.5492       100\n",
      "          22     0.3467    0.2600    0.2971       100\n",
      "          23     0.3901    0.7100    0.5035       100\n",
      "          24     0.5766    0.6400    0.6066       100\n",
      "          25     0.3750    0.1200    0.1818       100\n",
      "          26     0.4615    0.1800    0.2590       100\n",
      "          27     0.1529    0.2600    0.1926       100\n",
      "          28     0.6854    0.6100    0.6455       100\n",
      "          29     0.3053    0.2900    0.2974       100\n",
      "          30     0.3226    0.5000    0.3922       100\n",
      "          31     0.2979    0.4200    0.3485       100\n",
      "          32     0.2636    0.2900    0.2762       100\n",
      "          33     0.3630    0.4900    0.4170       100\n",
      "          34     0.3191    0.3000    0.3093       100\n",
      "          35     0.3333    0.1900    0.2420       100\n",
      "          36     0.4316    0.4100    0.4205       100\n",
      "          37     0.4557    0.3600    0.4022       100\n",
      "          38     0.2323    0.2300    0.2312       100\n",
      "          39     0.7308    0.3800    0.5000       100\n",
      "          40     0.4754    0.2900    0.3602       100\n",
      "          41     0.7317    0.6000    0.6593       100\n",
      "          42     0.3503    0.5500    0.4280       100\n",
      "          43     0.2654    0.5600    0.3601       100\n",
      "          44     0.2258    0.1400    0.1728       100\n",
      "          45     0.1524    0.1600    0.1561       100\n",
      "          46     0.2061    0.2700    0.2338       100\n",
      "          47     0.4364    0.4800    0.4571       100\n",
      "          48     0.6509    0.6900    0.6699       100\n",
      "          49     0.3711    0.7200    0.4898       100\n",
      "          50     0.2667    0.1600    0.2000       100\n",
      "          51     0.2857    0.2200    0.2486       100\n",
      "          52     0.4875    0.7800    0.6000       100\n",
      "          53     0.7097    0.6600    0.6839       100\n",
      "          54     0.5851    0.5500    0.5670       100\n",
      "          55     0.1224    0.0600    0.0805       100\n",
      "          56     0.2969    0.5700    0.3904       100\n",
      "          57     0.4521    0.3300    0.3815       100\n",
      "          58     0.5256    0.4100    0.4607       100\n",
      "          59     0.1935    0.3600    0.2517       100\n",
      "          60     0.7444    0.6700    0.7053       100\n",
      "          61     0.5556    0.5500    0.5528       100\n",
      "          62     0.3676    0.5000    0.4237       100\n",
      "          63     0.2109    0.5400    0.3034       100\n",
      "          64     0.1176    0.1000    0.1081       100\n",
      "          65     0.2941    0.1000    0.1493       100\n",
      "          66     0.2782    0.3700    0.3176       100\n",
      "          67     0.3448    0.2000    0.2532       100\n",
      "          68     0.5956    0.8100    0.6864       100\n",
      "          69     0.6484    0.5900    0.6178       100\n",
      "          70     0.4024    0.3300    0.3626       100\n",
      "          71     0.6500    0.5200    0.5778       100\n",
      "          72     0.1295    0.1800    0.1506       100\n",
      "          73     0.3267    0.3300    0.3284       100\n",
      "          74     0.2500    0.1100    0.1528       100\n",
      "          75     0.5197    0.6600    0.5815       100\n",
      "          76     0.5893    0.6600    0.6226       100\n",
      "          77     0.2289    0.1900    0.2077       100\n",
      "          78     0.1250    0.0300    0.0484       100\n",
      "          79     0.3333    0.3700    0.3507       100\n",
      "          80     0.2195    0.0900    0.1277       100\n",
      "          81     0.4130    0.3800    0.3958       100\n",
      "          82     0.4106    0.8500    0.5537       100\n",
      "          83     0.3208    0.3400    0.3301       100\n",
      "          84     0.4500    0.1800    0.2571       100\n",
      "          85     0.4483    0.6500    0.5306       100\n",
      "          86     0.6618    0.4500    0.5357       100\n",
      "          87     0.6250    0.3500    0.4487       100\n",
      "          88     0.4074    0.3300    0.3646       100\n",
      "          89     0.3646    0.3500    0.3571       100\n",
      "          90     0.2941    0.3500    0.3196       100\n",
      "          91     0.5234    0.5600    0.5411       100\n",
      "          92     0.2727    0.2400    0.2553       100\n",
      "          93     0.1761    0.2500    0.2066       100\n",
      "          94     0.8684    0.6600    0.7500       100\n",
      "          95     0.4767    0.4100    0.4409       100\n",
      "          96     0.2636    0.2900    0.2762       100\n",
      "          97     0.4430    0.3500    0.3911       100\n",
      "          98     0.2143    0.1500    0.1765       100\n",
      "          99     0.4035    0.2300    0.2930       100\n",
      "\n",
      "    accuracy                         0.3877     10000\n",
      "   macro avg     0.4080    0.3877    0.3819     10000\n",
      "weighted avg     0.4080    0.3877    0.3819     10000\n",
      "\n",
      "F1 score plot saved to: plots_learnable_loss_detailed_pytorch/f1_scores_cifar100_pytorch.png\n",
      "--- Completed processing CIFAR100 in 2053.23 seconds ---\n",
      "\n",
      "==================== Processing Dataset: OXFORD_IIIT_PET ====================\n",
      "Loading and preprocessing data...\n",
      "Data loading complete. Train samples: 3680, Test samples: 3669\n",
      "\n",
      "--- Starting Run 1/3 for OXFORD_IIIT_PET ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.3137 | Test Acc: 2.6710% | Test Task Loss: 0.1250 | Time: 16.99s\n",
      "  Epoch 002/20 | Train Total Loss: 0.1330 | Test Acc: 2.7255% | Test Task Loss: 0.1243 | Time: 14.67s\n",
      "  Epoch 003/20 | Train Total Loss: 0.1274 | Test Acc: 2.6710% | Test Task Loss: 0.1243 | Time: 14.93s\n",
      "  Epoch 004/20 | Train Total Loss: 0.1262 | Test Acc: 2.5893% | Test Task Loss: 0.1243 | Time: 14.91s\n",
      "  Epoch 005/20 | Train Total Loss: 0.1256 | Test Acc: 2.6710% | Test Task Loss: 0.1243 | Time: 14.86s\n",
      "  Epoch 006/20 | Train Total Loss: 0.1261 | Test Acc: 2.6710% | Test Task Loss: 0.1243 | Time: 14.86s\n",
      "  Epoch 007/20 | Train Total Loss: 0.1255 | Test Acc: 2.6710% | Test Task Loss: 0.1243 | Time: 14.72s\n",
      "  Epoch 008/20 | Train Total Loss: 0.1253 | Test Acc: 2.6710% | Test Task Loss: 0.1243 | Time: 14.66s\n",
      "  Epoch 009/20 | Train Total Loss: 0.1253 | Test Acc: 2.9436% | Test Task Loss: 0.1242 | Time: 14.97s\n",
      "  Epoch 010/20 | Train Total Loss: 0.1249 | Test Acc: 3.5432% | Test Task Loss: 0.1242 | Time: 14.97s\n",
      "  Epoch 011/20 | Train Total Loss: 0.1248 | Test Acc: 3.8975% | Test Task Loss: 0.1241 | Time: 15.07s\n",
      "  Epoch 012/20 | Train Total Loss: 0.1247 | Test Acc: 3.1344% | Test Task Loss: 0.1242 | Time: 15.24s\n",
      "  Epoch 013/20 | Train Total Loss: 0.1248 | Test Acc: 3.5977% | Test Task Loss: 0.1243 | Time: 14.90s\n",
      "  Epoch 014/20 | Train Total Loss: 0.1246 | Test Acc: 3.3252% | Test Task Loss: 0.1242 | Time: 14.73s\n",
      "  Epoch 015/20 | Train Total Loss: 0.1243 | Test Acc: 3.2434% | Test Task Loss: 0.1241 | Time: 14.81s\n",
      "  Epoch 016/20 | Train Total Loss: 0.1243 | Test Acc: 3.0526% | Test Task Loss: 0.1241 | Time: 14.63s\n",
      "  Epoch 017/20 | Train Total Loss: 0.1246 | Test Acc: 3.6795% | Test Task Loss: 0.1242 | Time: 14.63s\n",
      "  Epoch 018/20 | Train Total Loss: 0.1244 | Test Acc: 3.1889% | Test Task Loss: 0.1241 | Time: 14.70s\n",
      "  Epoch 019/20 | Train Total Loss: 0.1241 | Test Acc: 3.7067% | Test Task Loss: 0.1240 | Time: 14.79s\n",
      "  Epoch 020/20 | Train Total Loss: 0.1243 | Test Acc: 3.5977% | Test Task Loss: 0.1240 | Time: 14.78s\n",
      "--- Run 1 completed in 299.02 seconds. Final Test Accuracy: 3.5977% ---\n",
      "\n",
      "--- Starting Run 2/3 for OXFORD_IIIT_PET ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.3836 | Test Acc: 2.8618% | Test Task Loss: 0.1403 | Time: 14.63s\n",
      "  Epoch 002/20 | Train Total Loss: 0.2143 | Test Acc: 3.1344% | Test Task Loss: 0.1245 | Time: 14.75s\n",
      "  Epoch 003/20 | Train Total Loss: 0.1261 | Test Acc: 3.1071% | Test Task Loss: 0.1243 | Time: 15.09s\n",
      "  Epoch 004/20 | Train Total Loss: 0.1254 | Test Acc: 2.2077% | Test Task Loss: 0.1243 | Time: 15.04s\n",
      "  Epoch 005/20 | Train Total Loss: 0.1257 | Test Acc: 2.0442% | Test Task Loss: 0.1242 | Time: 14.84s\n",
      "  Epoch 006/20 | Train Total Loss: 0.1253 | Test Acc: 3.1344% | Test Task Loss: 0.1242 | Time: 14.77s\n",
      "  Epoch 007/20 | Train Total Loss: 0.1250 | Test Acc: 2.4257% | Test Task Loss: 0.1242 | Time: 14.95s\n",
      "  Epoch 008/20 | Train Total Loss: 0.1250 | Test Acc: 2.2349% | Test Task Loss: 0.1243 | Time: 15.13s\n",
      "  Epoch 009/20 | Train Total Loss: 0.1247 | Test Acc: 2.1259% | Test Task Loss: 0.1243 | Time: 15.06s\n",
      "  Epoch 010/20 | Train Total Loss: 0.1243 | Test Acc: 2.2349% | Test Task Loss: 0.1243 | Time: 14.75s\n",
      "  Epoch 011/20 | Train Total Loss: 0.1241 | Test Acc: 2.3440% | Test Task Loss: 0.1243 | Time: 15.00s\n",
      "  Epoch 012/20 | Train Total Loss: 0.1240 | Test Acc: 2.4802% | Test Task Loss: 0.1243 | Time: 15.26s\n",
      "  Epoch 013/20 | Train Total Loss: 0.1239 | Test Acc: 3.0526% | Test Task Loss: 0.1243 | Time: 15.36s\n",
      "  Epoch 014/20 | Train Total Loss: 0.1236 | Test Acc: 3.1616% | Test Task Loss: 0.1243 | Time: 14.94s\n",
      "  Epoch 015/20 | Train Total Loss: 0.1236 | Test Acc: 3.2161% | Test Task Loss: 0.1243 | Time: 15.31s\n",
      "  Epoch 016/20 | Train Total Loss: 0.1237 | Test Acc: 3.1616% | Test Task Loss: 0.1243 | Time: 15.26s\n",
      "  Epoch 017/20 | Train Total Loss: 0.1231 | Test Acc: 3.2434% | Test Task Loss: 0.1243 | Time: 15.02s\n",
      "  Epoch 018/20 | Train Total Loss: 0.1229 | Test Acc: 3.1616% | Test Task Loss: 0.1243 | Time: 15.22s\n",
      "  Epoch 019/20 | Train Total Loss: 0.1228 | Test Acc: 3.2161% | Test Task Loss: 0.1243 | Time: 14.89s\n",
      "  Epoch 020/20 | Train Total Loss: 0.1229 | Test Acc: 3.4069% | Test Task Loss: 0.1242 | Time: 14.90s\n",
      "--- Run 2 completed in 300.42 seconds. Final Test Accuracy: 3.4069% ---\n",
      "\n",
      "--- Starting Run 3/3 for OXFORD_IIIT_PET ---\n",
      "  Epoch 001/20 | Train Total Loss: 0.4493 | Test Acc: 2.7255% | Test Task Loss: 0.1711 | Time: 15.19s\n",
      "  Epoch 002/20 | Train Total Loss: 0.4475 | Test Acc: 2.7255% | Test Task Loss: 0.1279 | Time: 15.00s\n",
      "  Epoch 003/20 | Train Total Loss: 0.2151 | Test Acc: 3.1889% | Test Task Loss: 0.1244 | Time: 15.05s\n",
      "  Epoch 004/20 | Train Total Loss: 0.1287 | Test Acc: 3.2434% | Test Task Loss: 0.1243 | Time: 15.51s\n",
      "  Epoch 005/20 | Train Total Loss: 0.1259 | Test Acc: 3.1071% | Test Task Loss: 0.1243 | Time: 14.99s\n",
      "  Epoch 006/20 | Train Total Loss: 0.1267 | Test Acc: 2.5893% | Test Task Loss: 0.1243 | Time: 15.05s\n",
      "  Epoch 007/20 | Train Total Loss: 0.1261 | Test Acc: 2.4530% | Test Task Loss: 0.1243 | Time: 15.00s\n",
      "  Epoch 008/20 | Train Total Loss: 0.1255 | Test Acc: 2.5348% | Test Task Loss: 0.1243 | Time: 14.84s\n",
      "  Epoch 009/20 | Train Total Loss: 0.1254 | Test Acc: 2.7800% | Test Task Loss: 0.1243 | Time: 15.02s\n",
      "  Epoch 010/20 | Train Total Loss: 0.1251 | Test Acc: 2.7528% | Test Task Loss: 0.1243 | Time: 15.07s\n",
      "  Epoch 011/20 | Train Total Loss: 0.1250 | Test Acc: 2.6710% | Test Task Loss: 0.1243 | Time: 14.89s\n",
      "  Epoch 012/20 | Train Total Loss: 0.1254 | Test Acc: 2.7528% | Test Task Loss: 0.1243 | Time: 15.02s\n",
      "  Epoch 013/20 | Train Total Loss: 0.1250 | Test Acc: 2.8891% | Test Task Loss: 0.1243 | Time: 15.24s\n",
      "  Epoch 014/20 | Train Total Loss: 0.1249 | Test Acc: 2.7800% | Test Task Loss: 0.1243 | Time: 15.19s\n",
      "  Epoch 015/20 | Train Total Loss: 0.1247 | Test Acc: 2.8346% | Test Task Loss: 0.1243 | Time: 15.26s\n",
      "  Epoch 016/20 | Train Total Loss: 0.1248 | Test Acc: 2.8073% | Test Task Loss: 0.1243 | Time: 15.31s\n",
      "  Epoch 017/20 | Train Total Loss: 0.1246 | Test Acc: 2.8346% | Test Task Loss: 0.1243 | Time: 15.02s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd.functional import jacobian # For potential Hessian calculation alternative\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime # Although not used in the original logic, kept import if needed\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import classification_report # additional classification metrics\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed (e.g., UserWarnings from matplotlib)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # Might add these for potentially better reproducibility, but can slow down training\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Configuration & Hyperparameters ---\n",
    "LEARNING_RATE_CLASSIFIER = 0.001\n",
    "LEARNING_RATE_LOSS_NETWORK = 0.001\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 20  # Adjust as needed\n",
    "LAMBDA_CONST = 0.1  # Weight for the constraint losses\n",
    "NUM_RUNS = 3        # Number of runs for averaging\n",
    "PLOT_SAVE_DIR = \"plots_learnable_loss_detailed_pytorch\" # Directory to save plots\n",
    "\n",
    "# --- Device Selection ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Dataset Configurations (Adapted for PyTorch/Torchvision) ---\n",
    "DATASETS = {\n",
    "    \"mnist\": {\n",
    "        \"name\": \"mnist\",\n",
    "        \"load_fn\": torchvision.datasets.MNIST,\n",
    "        \"input_shape\": (1, 28, 28), # PyTorch: C, H, W\n",
    "        \"num_classes\": 10,\n",
    "        \"target_transform\": None # Labels are already integers\n",
    "    },\n",
    "    \"fashion_mnist\": {\n",
    "        \"name\": \"fashion_mnist\",\n",
    "        \"load_fn\": torchvision.datasets.FashionMNIST,\n",
    "        \"input_shape\": (1, 28, 28),\n",
    "        \"num_classes\": 10,\n",
    "         \"target_transform\": None\n",
    "    },\n",
    "    \"cifar10\": {\n",
    "        \"name\": \"cifar10\",\n",
    "        \"load_fn\": torchvision.datasets.CIFAR10,\n",
    "        \"input_shape\": (3, 32, 32),\n",
    "        \"num_classes\": 10,\n",
    "         \"target_transform\": None\n",
    "    },\n",
    "    \"cifar100\": {\n",
    "        \"name\": \"cifar100\",\n",
    "        \"load_fn\": torchvision.datasets.CIFAR100,\n",
    "        \"input_shape\": (3, 32, 32),\n",
    "        \"num_classes\": 100,\n",
    "         \"target_transform\": None\n",
    "    },\n",
    "    \"oxford_iiit_pet\": {\n",
    "        \"name\": \"oxford_iiit_pet\",\n",
    "        \"load_fn\": torchvision.datasets.OxfordIIITPet,\n",
    "        \"input_shape\": (3, 128, 128), # Resize target\n",
    "        \"num_classes\": 37,\n",
    "        # Labels are 0-36, already suitable for CrossEntropyLoss if used directly\n",
    "        \"target_transform\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Preprocessing Function (using torchvision.transforms) ---\n",
    "def get_transforms(input_shape):\n",
    "    \"\"\"Returns appropriate transforms for training and testing.\"\"\"\n",
    "    img_size = input_shape[1:] # H, W\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor() # Converts to Tensor, scales to [0,1], changes to C, H, W\n",
    "        # Add normalization if needed, e.g., for CIFAR:\n",
    "        # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# --- Model Building Functions (PyTorch nn.Module) ---\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        C, H, W = input_shape\n",
    "\n",
    "        self.conv1 = nn.Conv2d(C, 32, kernel_size=3, padding='same')\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        H //= 2; W //= 2\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding='same')\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        H //= 2; W //= 2\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding='same')\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        # No pooling after conv3\n",
    "\n",
    "        flattened_size = 128 * H * W\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(flattened_size, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        # IMPORTANT: Output softmax probabilities as expected by the TF loss logic\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = F.relu(self.bn3(self.conv3(x))) # No pool here\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # Apply softmax ONLY if not using nn.CrossEntropyLoss directly\n",
    "        # In this case, the custom loss needs probabilities, so we apply it.\n",
    "        p = self.softmax(x)\n",
    "        return p\n",
    "\n",
    "class LossNetwork(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LossNetwork, self).__init__()\n",
    "        input_dim = num_classes * 2\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64) # Use BatchNorm1d for FC layers\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1) # Output scalar energy\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is expected to be concatenation of p and y_one_hot\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        energy = self.fc4(x) # No activation on final output\n",
    "        return energy\n",
    "\n",
    "# --- Helper for Hessian Trace (using autograd.grad) ---\n",
    "def get_hessian_trace(grad_E, p):\n",
    "    \"\"\" Computes trace(d(grad_E)/dp) using autograd loops. \"\"\"\n",
    "    trace_val = 0.\n",
    "    # Need to ensure p has requires_grad=True for this\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "    for i in range(p.shape[1]): # Iterate through classes / columns of p\n",
    "        # Gradient of the i-th element of grad_E w.r.t p\n",
    "        # retain_graph=True needed because we reuse parts of the graph in the loop\n",
    "        # create_graph=True needed because the trace itself might be part of a larger loss graph\n",
    "        grad_grad_E_i = torch.autograd.grad(grad_E[:, i].sum(), p, create_graph=True, retain_graph=True)[0]\n",
    "        trace_val += grad_grad_E_i[:, i] # Add the diagonal element\n",
    "\n",
    "    # Detach p's requires_grad if it was set internally\n",
    "    # p.requires_grad_(False) # Be careful if p is needed later with grad\n",
    "    return trace_val\n",
    "\n",
    "# --- Training Step Function (Combined logic) ---\n",
    "# No separate function creation needed, integrated into epoch loop\n",
    "\n",
    "# --- Evaluation Step Function (Combined logic) ---\n",
    "# No separate function creation needed, integrated into epoch loop\n",
    "\n",
    "# --- Detailed Plotting Function ---\n",
    "def plot_detailed_learnable_loss_metrics(\n",
    "    dataset_name, epochs, num_runs, save_dir, num_classes,\n",
    "    mean_train_total_loss, std_train_total_loss,\n",
    "    mean_test_loss, std_test_loss,\n",
    "    mean_test_acc, std_test_acc,\n",
    "    mean_nonneg_loss, mean_convex_loss, mean_lips_loss,\n",
    "    final_loss_network_state # Pass state dict\n",
    "    ):\n",
    "    \"\"\"Generates and saves a 4-panel plot for learnable loss results.\"\"\"\n",
    "    epoch_range = range(1, epochs + 1)\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    plt.suptitle(f'{dataset_name.upper()} - Detailed Learnable Loss Metrics (Averaged over {num_runs} Runs) (PyTorch)',\n",
    "                 fontsize=16, y=1.02)\n",
    "\n",
    "    # (a) Training Total Loss / Validation Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epoch_range, mean_train_total_loss, label='Mean Training Total Loss', color='purple')\n",
    "    plt.fill_between(epoch_range, mean_train_total_loss - std_train_total_loss, mean_train_total_loss + std_train_total_loss,\n",
    "                     alpha=0.2, color='purple')\n",
    "    plt.plot(epoch_range, mean_test_loss, label='Mean Validation Task Loss', color='orange') # Label clarification\n",
    "    plt.fill_between(epoch_range, mean_test_loss - std_test_loss, mean_test_loss + std_test_loss,\n",
    "                     alpha=0.2, color='orange')\n",
    "    plt.title(\"Avg. Training Total vs Validation Task Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # (b) Validation Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epoch_range, mean_test_acc, label='Mean Validation Accuracy', color='blue')\n",
    "    plt.fill_between(epoch_range, mean_test_acc - std_test_acc, mean_test_acc + std_test_acc,\n",
    "                     alpha=0.2, color='blue')\n",
    "    plt.title(\"Avg. Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # (c) Constraint Penalties (log scale)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epoch_range, mean_nonneg_loss, label=\"Avg. Non-negativity Loss\", marker='.')\n",
    "    plt.plot(epoch_range, mean_convex_loss, label=\"Avg. Convexity Loss\", marker='.')\n",
    "    plt.plot(epoch_range, mean_lips_loss, label=\"Avg. Lipschitz Loss\", marker='.')\n",
    "    # Handle potential zero or negative values before log scale\n",
    "    min_positive = min(m for m in (np.min(mean_nonneg_loss), np.min(mean_convex_loss), np.min(mean_lips_loss)) if m > 0)\n",
    "    if min_positive > 0:\n",
    "         plt.ylim(bottom=min_positive * 0.1) # Set lower limit for log scale\n",
    "         plt.yscale('log')\n",
    "    else:\n",
    "        print(\"Warning: Non-positive values encountered in constraint losses, using linear scale.\")\n",
    "\n",
    "    plt.title(\"Avg. Constraint Penalties\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Avg. Penalty Value\" + (\" (log scale)\" if min_positive > 0 else \"\"))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # (d) Learned Loss Function Visualization (from last run)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    energies = []\n",
    "    n_points = 50\n",
    "    p_true_class_values = np.linspace(1e-3, 1.0 - 1e-3, n_points)\n",
    "    y_fixed = np.zeros((1, num_classes), dtype=np.float32)\n",
    "    y_fixed[0, 0] = 1.0 # Assume true class is 0 for visualization\n",
    "    y_fixed_tensor = torch.tensor(y_fixed).to(DEVICE)\n",
    "\n",
    "    # Load the final loss network\n",
    "    loss_vis_net = LossNetwork(num_classes).to(DEVICE)\n",
    "    loss_vis_net.load_state_dict(final_loss_network_state)\n",
    "    loss_vis_net.eval()\n",
    "\n",
    "    if num_classes > 1:\n",
    "        remaining_prob = (1.0 - p_true_class_values) / (num_classes - 1)\n",
    "    else:\n",
    "        remaining_prob = np.zeros_like(p_true_class_values) # Handle binary case\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, p0 in enumerate(p_true_class_values):\n",
    "            p_np = np.full((1, num_classes), remaining_prob[i], dtype=np.float32)\n",
    "            p_np[0, 0] = p0\n",
    "            # Normalize p_np to ensure it sums to 1\n",
    "            p_np = p_np / np.sum(p_np, axis=1, keepdims=True)\n",
    "            p_tensor = torch.tensor(p_np).to(DEVICE)\n",
    "\n",
    "            loss_net_input = torch.cat([p_tensor, y_fixed_tensor], dim=1)\n",
    "            try:\n",
    "                E_val = loss_vis_net(loss_net_input)\n",
    "                energies.append(E_val.cpu().numpy().squeeze())\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error during loss visualization: {e}\")\n",
    "                energies.append(np.nan)\n",
    "\n",
    "    energies = np.array(energies) # Ensure it's a numpy array\n",
    "    if not np.all(np.isnan(energies)) and len(energies) > 0:\n",
    "        plt.plot(p_true_class_values, energies, marker='.')\n",
    "        min_energy_idx = np.nanargmin(energies)\n",
    "        if not np.isnan(energies[min_energy_idx]):\n",
    "            plt.scatter(p_true_class_values[min_energy_idx], energies[min_energy_idx], color='red', zorder=5,\n",
    "                        label=f'Min E at p[0]~{p_true_class_values[min_energy_idx]:.2f}')\n",
    "        plt.legend()\n",
    "    else:\n",
    "         plt.text(0.5, 0.5, 'Visualization Error or No Data', horizontalalignment='center',\n",
    "                  verticalalignment='center', transform=plt.gca().transAxes)\n",
    "\n",
    "    plt.title(f\"Learned Loss E(p,y) vs p[true_class=0] (Last Run)\")\n",
    "    plt.xlabel(\"p[0] (Probability for true class 0)\")\n",
    "    plt.ylabel(\"Energy E(p,y)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    try:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = os.path.join(save_dir, f\"results_{dataset_name}_learnable_loss_detailed_plots_pytorch.png\")\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Detailed plot saved to: {filename}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error saving detailed plot to {save_dir}: {e}\")\n",
    "    plt.close() # Close the figure\n",
    "\n",
    "\n",
    "# --- F1 Score Plotting Function ---\n",
    "def plot_f1_scores(dataset_name, save_dir, report_dict, num_classes):\n",
    "    \"\"\" Generates and saves a bar plot of per-class F1 scores. \"\"\"\n",
    "    class_labels = [str(i) for i in range(num_classes)]\n",
    "    f1_scores = [report_dict.get(label, {}).get('f1-score', 0) for label in class_labels]\n",
    "\n",
    "    plt.figure(figsize=(max(8, num_classes * 0.3), 5)) # Adjust width based on num_classes\n",
    "    bars = plt.bar(class_labels, f1_scores, color='dodgerblue', alpha=0.8)\n",
    "    plt.xlabel(\"Class Label\")\n",
    "    plt.ylabel(\"F1-Score\")\n",
    "    plt.title(f\"Per-Class F1 Scores for {dataset_name.upper()} (Last Run) (PyTorch)\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.6, axis='y')\n",
    "    plt.ylim(0, 1.05)\n",
    "\n",
    "    # Add text labels on bars if few classes, otherwise skip\n",
    "    if num_classes <= 20:\n",
    "        for bar in bars:\n",
    "             yval = bar.get_height()\n",
    "             plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.2f}', va='bottom', ha='center') # Add text labels\n",
    "\n",
    "    if num_classes > 20: # Rotate labels if too many classes\n",
    "         plt.xticks(rotation=90, fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    f1_plot_filename = os.path.join(save_dir, f\"f1_scores_{dataset_name}_pytorch.png\")\n",
    "    try:\n",
    "        plt.savefig(f1_plot_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"F1 score plot saved to: {f1_plot_filename}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error saving F1 score plot: {e}\")\n",
    "    plt.close() # Close the figure\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Block\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # GPU check is done globally via DEVICE definition\n",
    "\n",
    "    # Create plot directory\n",
    "    try:\n",
    "        os.makedirs(PLOT_SAVE_DIR, exist_ok=True)\n",
    "        print(f\"Plots will be saved in '{PLOT_SAVE_DIR}/' directory.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Could not create plot directory '{PLOT_SAVE_DIR}': {e}. Plots will fallback to current directory.\")\n",
    "        PLOT_SAVE_DIR = \".\"\n",
    "\n",
    "    # Loop through Datasets\n",
    "    for dataset_key, config in DATASETS.items():\n",
    "        print(f\"\\n{'='*20} Processing Dataset: {dataset_key.upper()} {'='*20}\")\n",
    "        start_time_dataset = time.time()\n",
    "\n",
    "        input_shape = config[\"input_shape\"]\n",
    "        num_classes = config[\"num_classes\"]\n",
    "        DatasetClass = config[\"load_fn\"]\n",
    "        target_transform = config[\"target_transform\"]\n",
    "\n",
    "        # Data Loading & Preprocessing\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        transform = get_transforms(input_shape)\n",
    "\n",
    "        # Handle different dataset loading arguments\n",
    "        try:\n",
    "            if dataset_key == \"oxford_iiit_pet\":\n",
    "                train_dataset = DatasetClass(root='./data', split='trainval', download=True, transform=transform, target_transform=target_transform)\n",
    "                test_dataset = DatasetClass(root='./data', split='test', download=True, transform=transform, target_transform=target_transform)\n",
    "            else:\n",
    "                train_dataset = DatasetClass(root='./data', train=True, download=True, transform=transform, target_transform=target_transform)\n",
    "                test_dataset = DatasetClass(root='./data', train=False, download=True, transform=transform, target_transform=target_transform)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "            print(f\"Data loading complete. Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to load dataset {dataset_key}. Skipping. Error: {e}\")\n",
    "            continue # Skip to next dataset\n",
    "\n",
    "        # Multiple Runs Loop\n",
    "        all_runs_train_total_loss_hist = []\n",
    "        all_runs_train_nonneg_loss_hist = []\n",
    "        all_runs_train_convex_loss_hist = []\n",
    "        all_runs_train_lips_loss_hist = []\n",
    "        all_runs_test_loss_hist = [] # Stores task loss on test set\n",
    "        all_runs_test_acc_hist = []\n",
    "        run_times = []\n",
    "        final_loss_network_state_last_run = None  # For visualization\n",
    "        final_classifier_state_last_run = None    # For final metrics/plot\n",
    "\n",
    "        for run in range(NUM_RUNS):\n",
    "            print(f\"\\n--- Starting Run {run + 1}/{NUM_RUNS} for {dataset_key.upper()} ---\")\n",
    "            start_time_run = time.time()\n",
    "\n",
    "            # Build new models and optimizers\n",
    "            classifier = Classifier(input_shape, num_classes).to(DEVICE)\n",
    "            loss_network = LossNetwork(num_classes).to(DEVICE)\n",
    "            optimizer_classifier = optim.Adam(classifier.parameters(), lr=LEARNING_RATE_CLASSIFIER)\n",
    "            optimizer_loss_network = optim.Adam(loss_network.parameters(), lr=LEARNING_RATE_LOSS_NETWORK)\n",
    "\n",
    "            # Task loss for evaluation (using probabilities and one-hot)\n",
    "            # BinaryCrossEntropy is suitable here as p is softmax and y is one-hot\n",
    "            eval_task_loss_fn = nn.BCELoss()\n",
    "\n",
    "            run_train_total_losses = []\n",
    "            run_train_nonneg_losses = []\n",
    "            run_train_convex_losses = []\n",
    "            run_train_lips_losses = []\n",
    "            run_test_losses = []\n",
    "            run_test_accuracies = []\n",
    "\n",
    "            for epoch in range(EPOCHS):\n",
    "                epoch_start_time = time.time()\n",
    "                # --- Training Phase ---\n",
    "                classifier.train()\n",
    "                loss_network.train()\n",
    "                epoch_train_total_loss = 0.0\n",
    "                epoch_train_nonneg_loss = 0.0\n",
    "                epoch_train_convex_loss = 0.0\n",
    "                epoch_train_lips_loss = 0.0\n",
    "                train_batches = 0\n",
    "\n",
    "                for x_batch, y_batch_labels in train_loader:\n",
    "                    x_batch = x_batch.to(DEVICE)\n",
    "                    # Labels are integers, convert to one-hot for loss network input & BCE task loss\n",
    "                    y_one_hot = F.one_hot(y_batch_labels, num_classes=num_classes).float().to(DEVICE)\n",
    "\n",
    "                    # Zero gradients\n",
    "                    optimizer_classifier.zero_grad()\n",
    "                    optimizer_loss_network.zero_grad()\n",
    "\n",
    "                    # Forward pass classifier\n",
    "                    p = classifier(x_batch)\n",
    "                    p = torch.clamp(p, 1e-7, 1.0 - 1e-7) # Clamp for numerical stability with BCE/log\n",
    "\n",
    "                    # Prepare input for loss network\n",
    "                    loss_net_input = torch.cat((p, y_one_hot), dim=1)\n",
    "                    # Ensure input requires grad for gradient calculation w.r.t. itself\n",
    "                    loss_net_input.requires_grad_(True)\n",
    "\n",
    "                    # Forward pass loss network\n",
    "                    E = loss_network(loss_net_input) # Energy output\n",
    "\n",
    "                    # Calculate grad_E w.r.t loss_net_input\n",
    "                    # create_graph=True needed for Hessian calculation later\n",
    "                    grad_E_full = torch.autograd.grad(E.sum(), loss_net_input, create_graph=True)[0]\n",
    "                    grad_E = grad_E_full[:, :num_classes] # Extract gradient w.r.t p\n",
    "\n",
    "                    # Calculate Hessian trace (trace(d(grad_E)/dp))\n",
    "                    # Ensure p requires grad for this step\n",
    "                    if not p.requires_grad:\n",
    "                         p.requires_grad_(True) # Should be set by default if params require grad\n",
    "                    hessian_trace = get_hessian_trace(grad_E, p)\n",
    "\n",
    "\n",
    "                    # Calculate loss components\n",
    "                    L_task = eval_task_loss_fn(p, y_one_hot) # Task loss (BCE on probs/one-hot)\n",
    "                    L_nonneg = torch.mean(torch.square(F.relu(-E)))\n",
    "                    L_convex = torch.mean(torch.square(F.relu(-hessian_trace)))\n",
    "                    grad_norm = torch.linalg.norm(grad_E, dim=1)\n",
    "                    L_lips = torch.mean(torch.square(grad_norm - 1.0))\n",
    "                    L_constraint = L_nonneg + L_convex + L_lips\n",
    "\n",
    "                    # Total loss\n",
    "                    total_loss = L_task + LAMBDA_CONST * L_constraint\n",
    "\n",
    "                    # Backward pass (computes gradients for BOTH networks)\n",
    "                    total_loss.backward()\n",
    "\n",
    "                    # Optimize\n",
    "                    optimizer_classifier.step()\n",
    "                    optimizer_loss_network.step()\n",
    "\n",
    "                    # Accumulate epoch losses (use .item() to detach from graph)\n",
    "                    epoch_train_total_loss += total_loss.item()\n",
    "                    epoch_train_nonneg_loss += L_nonneg.item()\n",
    "                    epoch_train_convex_loss += L_convex.item()\n",
    "                    epoch_train_lips_loss += L_lips.item()\n",
    "                    train_batches += 1\n",
    "\n",
    "                    # --- Explicitly delete tensors and clear cache if memory issues arise ---\n",
    "                    # del loss_net_input, E, grad_E_full, grad_E, hessian_trace, total_loss\n",
    "                    # if DEVICE == 'cuda': torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "                # --- Evaluation Phase ---\n",
    "                classifier.eval()\n",
    "                loss_network.eval() # Not strictly needed if only classifier used, but good practice\n",
    "                epoch_test_loss = 0.0\n",
    "                correct_test = 0\n",
    "                total_test = 0\n",
    "                test_batches = 0\n",
    "                with torch.no_grad():\n",
    "                    for x_batch_test, y_batch_test_labels in test_loader:\n",
    "                        x_batch_test = x_batch_test.to(DEVICE)\n",
    "                        y_batch_test_labels = y_batch_test_labels.to(DEVICE)\n",
    "                        # Convert labels to one-hot for consistency with eval task loss function\n",
    "                        y_batch_test_one_hot = F.one_hot(y_batch_test_labels, num_classes=num_classes).float().to(DEVICE)\n",
    "\n",
    "                        p_test = classifier(x_batch_test)\n",
    "                        p_test = torch.clamp(p_test, 1e-7, 1.0 - 1e-7) # Clamp for BCE\n",
    "\n",
    "                        test_loss = eval_task_loss_fn(p_test, y_batch_test_one_hot)\n",
    "                        epoch_test_loss += test_loss.item()\n",
    "\n",
    "                        # Calculate accuracy using integer labels\n",
    "                        _, predicted = torch.max(p_test.data, 1)\n",
    "                        total_test += y_batch_test_labels.size(0)\n",
    "                        correct_test += (predicted == y_batch_test_labels).sum().item()\n",
    "                        test_batches += 1\n",
    "\n",
    "                # Calculate average losses and accuracy for the epoch\n",
    "                avg_train_total_loss = epoch_train_total_loss / train_batches\n",
    "                avg_train_nonneg_loss = epoch_train_nonneg_loss / train_batches\n",
    "                avg_train_convex_loss = epoch_train_convex_loss / train_batches\n",
    "                avg_train_lips_loss = epoch_train_lips_loss / train_batches\n",
    "                avg_test_loss = epoch_test_loss / test_batches\n",
    "                avg_test_acc = correct_test / total_test\n",
    "\n",
    "                run_train_total_losses.append(avg_train_total_loss)\n",
    "                run_train_nonneg_losses.append(avg_train_nonneg_loss)\n",
    "                run_train_convex_losses.append(avg_train_convex_loss)\n",
    "                run_train_lips_losses.append(avg_train_lips_loss)\n",
    "                run_test_losses.append(avg_test_loss)\n",
    "                run_test_accuracies.append(avg_test_acc)\n",
    "\n",
    "                epoch_duration = time.time() - epoch_start_time\n",
    "                print(f\"  Epoch {epoch+1:03d}/{EPOCHS} | Train Total Loss: {avg_train_total_loss:.4f} \"\n",
    "                      f\"| Test Acc: {avg_test_acc:.4%} | Test Task Loss: {avg_test_loss:.4f} | Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "            # --- End of Run ---\n",
    "            all_runs_train_total_loss_hist.append(run_train_total_losses)\n",
    "            all_runs_train_nonneg_loss_hist.append(run_train_nonneg_losses)\n",
    "            all_runs_train_convex_loss_hist.append(run_train_convex_losses)\n",
    "            all_runs_train_lips_loss_hist.append(run_train_lips_losses)\n",
    "            all_runs_test_loss_hist.append(run_test_losses)\n",
    "            all_runs_test_acc_hist.append(run_test_accuracies)\n",
    "\n",
    "            run_time = time.time() - start_time_run\n",
    "            run_times.append(run_time)\n",
    "            print(f\"--- Run {run + 1} completed in {run_time:.2f} seconds. Final Test Accuracy: {run_test_accuracies[-1]:.4%} ---\")\n",
    "            if run == NUM_RUNS - 1: # Save the state of the last run's models\n",
    "                final_loss_network_state_last_run = loss_network.state_dict()\n",
    "                final_classifier_state_last_run = classifier.state_dict()\n",
    "\n",
    "        # --- Aggregating and Plotting ---\n",
    "        def calc_mean_std(history_list, max_len):\n",
    "            # Ensure all lists are numpy arrays before padding\n",
    "            history_list_np = [np.array(hist) for hist in history_list]\n",
    "            # Pad sequences to max_len (EPOCHS)\n",
    "            padded_list = [np.pad(hist, (0, max_len - len(hist)), 'edge') if len(hist) < max_len else hist[:max_len] for hist in history_list_np]\n",
    "            if not padded_list: # Handle case where no runs completed\n",
    "                return np.full(max_len, np.nan), np.full(max_len, np.nan)\n",
    "            mean_vals = np.nanmean(padded_list, axis=0) # Use nanmean for safety\n",
    "            std_vals = np.nanstd(padded_list, axis=0)   # Use nanstd for safety\n",
    "            return mean_vals, std_vals\n",
    "\n",
    "        mean_train_total, std_train_total = calc_mean_std(all_runs_train_total_loss_hist, EPOCHS)\n",
    "        mean_test_loss, std_test_loss = calc_mean_std(all_runs_test_loss_hist, EPOCHS)\n",
    "        mean_test_acc, std_test_acc = calc_mean_std(all_runs_test_acc_hist, EPOCHS)\n",
    "        mean_nonneg, std_nonneg = calc_mean_std(all_runs_train_nonneg_loss_hist, EPOCHS)\n",
    "        mean_convex, std_convex = calc_mean_std(all_runs_train_convex_loss_hist, EPOCHS)\n",
    "        mean_lips, std_lips = calc_mean_std(all_runs_train_lips_loss_hist, EPOCHS)\n",
    "\n",
    "        # Print final averaged metrics (last epoch)\n",
    "        print(\"\\n=== Averaged Final Epoch Metrics (PyTorch) ===\")\n",
    "        # Check if metrics have valid values (not NaN) before printing\n",
    "        if not np.isnan(mean_train_total[-1]): print(f\"Train Total Loss: {mean_train_total[-1]:.4f} ± {std_train_total[-1]:.4f}\")\n",
    "        if not np.isnan(mean_test_loss[-1]): print(f\"Test Task Loss:   {mean_test_loss[-1]:.4f} ± {std_test_loss[-1]:.4f}\") # Clarified label\n",
    "        if not np.isnan(mean_test_acc[-1]): print(f\"Test Accuracy:    {mean_test_acc[-1]*100:.2f}% ± {std_test_acc[-1]*100:.2f}%\")\n",
    "        if not np.isnan(mean_nonneg[-1]): print(f\"Non-Negativity Loss:{mean_nonneg[-1]:.4f} ± {std_nonneg[-1]:.4f}\")\n",
    "        if not np.isnan(mean_convex[-1]): print(f\"Convexity Loss:   {mean_convex[-1]:.4f} ± {std_convex[-1]:.4f}\")\n",
    "        if not np.isnan(mean_lips[-1]): print(f\"Lipschitz Loss:   {mean_lips[-1]:.4f} ± {std_lips[-1]:.4f}\")\n",
    "        print(f\"Average Run Time: {np.mean(run_times):.2f} seconds\")\n",
    "\n",
    "        # Detailed plots (using last run's final loss network state)\n",
    "        if final_loss_network_state_last_run is not None:\n",
    "            plot_detailed_learnable_loss_metrics(\n",
    "                dataset_name=dataset_key, epochs=EPOCHS, num_runs=NUM_RUNS, save_dir=PLOT_SAVE_DIR,\n",
    "                num_classes=num_classes,\n",
    "                mean_train_total_loss=mean_train_total, std_train_total_loss=std_train_total,\n",
    "                mean_test_loss=mean_test_loss, std_test_loss=std_test_loss,\n",
    "                mean_test_acc=mean_test_acc, std_test_acc=std_test_acc,\n",
    "                mean_nonneg_loss=mean_nonneg, mean_convex_loss=mean_convex, mean_lips_loss=mean_lips,\n",
    "                final_loss_network_state=final_loss_network_state_last_run\n",
    "            )\n",
    "        else:\n",
    "            print(\"Warning: Could not generate detailed plot as final loss network state was not available.\")\n",
    "\n",
    "\n",
    "        # Compute additional classification metrics and F1 plot using the final classifier from the last run\n",
    "        classification_report_str = \"N/A\"\n",
    "        report_dict = {}\n",
    "        if final_classifier_state_last_run is not None:\n",
    "            print(\"\\n=== Generating Final Classification Report & F1 Plot (Last Run) ===\")\n",
    "            final_classifier = Classifier(input_shape, num_classes).to(DEVICE)\n",
    "            final_classifier.load_state_dict(final_classifier_state_last_run)\n",
    "            final_classifier.eval()\n",
    "\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for x_batch, y_batch_labels in test_loader:\n",
    "                    x_batch = x_batch.to(DEVICE)\n",
    "                    outputs = final_classifier(x_batch)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(y_batch_labels.cpu().numpy())\n",
    "\n",
    "            print(\"\\nClassification Metrics (sklearn):\")\n",
    "            # Use zero_division=0 to avoid warnings/errors for classes with no support\n",
    "            report_dict = classification_report(all_labels, all_preds, digits=4, output_dict=True, zero_division=0)\n",
    "            classification_report_str = classification_report(all_labels, all_preds, digits=4, zero_division=0)\n",
    "            print(classification_report_str)\n",
    "\n",
    "            # Generate and save F1 score plot\n",
    "            plot_f1_scores(dataset_key, PLOT_SAVE_DIR, report_dict, num_classes)\n",
    "\n",
    "        else:\n",
    "            print(\"Warning: No final classifier state available for computing final metrics or F1 plot.\")\n",
    "\n",
    "        end_time_dataset = time.time()\n",
    "        print(f\"--- Completed processing {dataset_key.upper()} in {end_time_dataset - start_time_dataset:.2f} seconds ---\")\n",
    "\n",
    "    print(f\"\\n{'='*20} All Dataset Processing Complete (PyTorch) {'='*20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f10c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
